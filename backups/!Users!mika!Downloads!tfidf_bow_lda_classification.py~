import codecs
import sys
import json
from pprint import pprint   # pretty-printer
from gensim import corpora, models
# from stemming.porter2 import stem
from nltk.stem.snowball import GermanStemmer, EnglishStemmer
from stop_words import get_stop_words
import pickle
from random import shuffle
from collections import OrderedDict
import numpy as np
from collections import Counter

# from sklearn.cross_validation import train_test_split, StratifiedKFold
from sklearn.preprocessing import Normalizer
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.base import TransformerMixin, BaseEstimator
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn import metrics
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold

from scipy import interp
# import matplotlib.pyplot as plt
from bokeh.plotting import figure, show


sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())



pkl_file = open('../data/e2e_eval.pkl', 'rb')
data = pickle.load(pkl_file)
#pprint(data)

language = "de"     #choose language of target messages
booking_pure= True  #for True only messages that contain only the topic "booking" count as booking
number_files = 0
messages = {}
labels = []
labels_en = []
labels_de = []
languages = []
count = 0   
for idx, elem in enumerate(data):
		#print(idx,elem['id'])
		# if True:
		#       continue    
		if idx>=3505:    #use only batch 1 and 2, ignore double messages, messages are ordered
		    break
		first=0
		last=-1
		location_spans=[]
		person_spans=[]
		spans = []
		subject_spans = []
		# print(elem,"elem")
		if 'annotations' in elem['pred_ann']:
			for attr in elem['pred_ann']['annotations']:
				# print(attr, "attr")
				
				if attr['type']=="EndOfMsg":
				   attr['spans'].sort(key=lambda x: x["start"], reverse=False)
				   last=attr['spans'][0]['start']
				if attr['type']=="Greeting":
				   attr['spans'].sort(key=lambda x: x["end"], reverse=True)  
				   first=attr['spans'][0]['end']   
				if attr['type']=="Location":   				
					attr['spans'][0]["type"]="Location"
					location_spans.append(attr['spans'])
					if attr['where']=="body":
					  spans.append(attr['spans'])
					elif attr['where']=="subject":
					  subject_spans.append(attr['spans'])  

				if attr['type']=="Person":   
					attr['spans'][0]["type"]="Person"
					person_spans.append(attr['spans'])
					if attr['where']=="body":
					  spans.append(attr['spans'])
					elif attr['where']=="subject":
					  subject_spans.append(attr['spans'])
					
					# print(attr['spans'],"attr['spans']")
				if attr['type']=="TimePoint":   
				
					attr['spans'][0]["type"]="Timepoint"
					if attr['where']=="body":
					  spans.append(attr['spans'])
					elif attr['where']=="subject":
					  subject_spans.append(attr['spans'])
					
					# print(attr['spans'],"attr['spans']")
				if attr['type']=="TimeRange":   
					
					attr['spans'][0]["type"]="Timerange"
					if attr['where']=="body":
					  spans.append(attr['spans'])
					elif attr['where']=="subject":
					  subject_spans.append(attr['spans'])
					
				
					
		labels.append(elem['ann']['label'])
		messages[count] = elem['msg']['body'][:last]
		
		if 'language' in elem['pred_ann']:
			if elem['pred_ann']['language'] == "en":
			   labels_en.append(elem['ann']['label'])
			   languages.append("en")
			elif elem['pred_ann']['language'] == "de":	
			   labels_de.append(elem['ann']['label'])
			   languages.append("de")
			else:
			   languages.append("other")  #ignoring other language labels for now
			   
		else:	
		       labels_de.append(elem['ann']['label']) #if no information available, guess it belongs to the biggest class (german)
		       languages.append("de")         
        
	
		
		#filter out spans that are in Greeting or EndOfMsg
		spans = [elem for elem in spans if elem[0]["end"]<last and elem[0]["start"]>first]
		spans.sort(key=lambda x: x[0]["start"], reverse=True)
		subject_spans.sort(key=lambda x: x[0]["start"], reverse=True)
		for elems in spans:
			messages[count] = messages[count][:elems[0]["start"]]+elems[0]["type"]+""+messages[count][elems[0]["end"]:]
			
		for elems in subject_spans:
			#messages[count][:elems[0]["start"]] +elems[0]["type"]+""+messages[count][elems[0]["end"]:]
			elem['msg']['subject']=elem['msg']['subject'][:elems[0]["start"]]+elems[0]["type"]+" "+elem['msg']['subject'][elems[0]["end"]:]
		messages[count] = messages[count] +" "+ elem['msg']['subject']
		messages[count] = messages[count][first:].translate({ord(c): ' ' for c in '[]*?>!#$()/_\'`'})
		# print(messages[count])
		count += 1

	
# pprint(messages)
#check message 280

# for idx, message in messages.items():
# 		# if idx>=300:    #use only batch 1 and 2, ignore double messages
# 		#    break
# 		print(data[idx]['msg']['body'], "original ",idx)
# 		print(message, "with replacements ",idx)

		
print(count, " documents")      #3505 messages, 7010 if all batches are used  
#correct wrong label names
for i in range(len(labels_de)):
	if labels_de[i]=="bookinglnegotiation":
	     labels_de[i]="booking|negotiation"
	elif labels_de[i]=="ebooking":
		 labels_de[i]="rebooking"

	
for i in range(len(labels_en)):
		if labels_en[i]=="negoatiation":
		  labels_en[i]="negotiation"
		elif labels_en[i]=="bookinglother":
		 labels_en[i]="booking|other"

print(set(labels_en), "set english labels")	
print(set(labels_de), "set german labels")	
print(Counter(labels_en),len(labels_en))
print(Counter(labels_de),len(labels_de))

#simplify labels, only booking and no booking, booking_pure:booking as only topic
if booking_pure:

	for i in range(len(labels_de)):
		if labels_de[i]=='booking':
		     pass
		else:
		     labels_de[i]="no booking"	 	 		 		 

		
	for i in range(len(labels_en)):
			if labels_en[i]=='booking':
			    pass 
			else:
			 labels_en[i]="no booking"
else:


	for i in range(len(labels_de)):
		if labels_de[i]=='booking|cancellation':
		     labels_de[i]="booking"
		elif labels_de[i]=="booking|negotiation":
			 labels_de[i]="booking"
		elif labels_de[i]=="booking|other":
			 labels_de[i]="booking"
		elif labels_de[i]=="booking|rebooking":
			 labels_de[i]="booking"
		elif labels_de[i]=="booking":
			 pass	
		else:
		     labels_de[i]="no booking"	 	 		 		 

		
	for i in range(len(labels_en)):
			if labels_en[i]=='booking|cancellation':
			  labels_en[i]="booking"
			elif labels_en[i]=='booking|negotiation':
			 labels_en[i]="booking"
			elif labels_en[i]=="booking|other":
			 labels_en[i]="booking" 
			elif labels_en[i]=="booking|rebooking":
			 labels_en[i]="booking"
			elif labels_en[i]=="booking":
			 pass	
			else:
			 labels_en[i]="no booking"



print(set(labels_en), "set english labels")	
print(set(labels_de), "set german labels")

if language == "de":
	labels = labels_de
elif language == "en":
    labels = labels_en

#  # remove common words
stop_words_eng = get_stop_words('en')
stop_words_de = get_stop_words('de')
# specific_words = ['bitte', 'please','gmbh','email','e-mail', 'message','management','management','berlin','amtsgericht','travel','+49','hallo','geschäftsführer','inform', 'comtravo','project','geschäftsführer:','sitz','vielen','dank','thank','thanks','2016','2015','partner','venture','capital','grüße','viele','venture','030','manage','freundlichen','fax','tel','may'] 
stop_words = stop_words_de + stop_words_eng 


texts = [[word for word in str(messages[key]).lower().split() if word not in stop_words and len(word)>2]
         for key, message in messages.items() if languages[key]==language]
# texts_ = [x for x in texts if x != []]         


# # remove words that appear only once gensim LDA
# from collections import defaultdict
# frequency = defaultdict(int)
# for text in texts:
#      for token in text:
#         frequency[token] += 1
 
# # print frequency
# texts = [[token for token in text if frequency[token] > 1]
#           for text in texts]
# dictionary = corpora.Dictionary(texts) 
#bag_of_words_features=2000 

#different vocabularies for bow
vocabulary_de_en=['fliegen','fly','book','booking','cancel','stornieren','umbuchen','rebook','cancellation','reservieren','reserve','going','neu',
'umbuchbar','stornierbar','anfrage','preis','preisangabe','storno','sorry','mietwagen','geb.','geburtstag','birthday','perso','führerschein','angebot','ticket', 
'eco','hotel', 'airbnb','persons','bedrooms','einzelzimmer','find','finden','raussuchen','nähe','close','far','distance','location',
'gebucht','booked','vortag','buchungsanfrage','übernachtung','nights','buchung','reservation','brauchen','need','sitzplatz','economy',
'business','class','klasse','kl','noch','premium','reasonable', 'absagen','verschieben','ort','schlafen', 'sleep', 'fahren','canceln',
'leid','falsch','korigieren','gebühr','nichtraucher', 'miles','personen','ankunft', 'arrival','departure', 'abflug', 'ab', 'an','timepoint','timerange','person',
'abholen','fenster','ruhebereich','helfen','wieder','airline','sparpreis','normalpreis','ändern','verlust','gangplatz','fensterplatz','vorne','hinten'
,'flug','gemeinsam','zusätzlich', 'passport','id','birthdate','birth','flughafen','bahnhof','train','zug','travel','go','hbf','flugzeit','dauer','duration','stops','fees','wagen','car','gate','terminal','zusammen','zwischenstopp','cheapest','cheap','billig','preiswert','günstig',
'abfahrt','verbindung','gleis']
vocabulary_de_en_reduced=['fliegen','fly','book','booking','cancel','stornieren','umbuchen','rebook','cancellation','reservieren','reserve','going','neu',
'umbuchbar','stornierbar','anfrage','preis','preisangabe','storno','mietwagen','geb.','geburtstag','birthday','perso','führerschein','angebot','ticket', 
'eco','hotel', 'airbnb','persons','bedrooms','einzelzimmer','find','finden','location',
'gebucht','booked','buchungsanfrage','übernachtung','nights','buchung','reservation','sitzplatz','economy',
'business','class','klasse','premium', 'absagen','verschieben','ort','schlafen', 'sleep', 'fahren','canceln',
'gebühr','nichtraucher', 'personen','ankunft', 'arrival','departure', 'abflug', 'ab', 'an','timepoint','timerange','person','fenster','ruhebereich','airline','sparpreis','normalpreis','ändern','gangplatz','fensterplatz',
'flug', 'passport','id','birthdate','birth','flughafen','bahnhof','train','zug','travel','hbf','flugzeit','dauer','duration','stops','fees','wagen','car','abfahrt']
vocabulary_de=['fliegen','fly','stornieren','umbuchen','reservieren','neu',
'umbuchbar','stornierbar','anfrage','preis','preisangabe','storno','sorry','mietwagen','geb.','geburtstag','perso','führerschein','angebot','ticket', 
'eco','hotel', 'airbnb','einzelzimmer','finden','raussuchen','nähe','location',
'gebucht','vortag','buchungsanfrage','übernachtung','buchung','brauchen','sitzplatz','economy',
'business','class','klasse','kl','noch','premium','absagen','verschieben','ort','schlafen', 'fahren','canceln',
'leid','falsch','korrigieren','gebühr','nichtraucher','personen','ankunft', 'abflug', 'ab', 'an','timepoint','timerange','person',
'abholen','fenster','ruhebereich','helfen','wieder','airline','sparpreis','normalpreis','ändern','verlust','gangplatz','fensterplatz','vorne','hinten'
,'flug','gemeinsam','zusätzlich', 'id','flughafen','bahnhof','zug','hbf','flugzeit','dauer','stops','wagen','gate','terminal','zusammen','zwischenstopp','billig','preiswert','günstig',
'abfahrt','verbindung','gleis']
vocabulary_de_reduced=['fliegen','stornieren','umbuchen','reservieren','neu',
'umbuchbar','stornierbar','anfrage','preis','preisangabe','storno','mietwagen','geb.','geburtstag','perso','führerschein','angebot','ticket', 
'eco','hotel', 'airbnb','bedrooms','einzelzimmer','finden','location',
'gebucht','buchungsanfrage','übernachtung','buchung','sitzplatz','economy',
'business','class','klasse','premium', 'absagen','verschieben','ort','schlafen', 'fahren','canceln',
'gebühr','nichtraucher', 'personen','ankunft', 'abflug', 'ab', 'an','timepoint','timerange','person','fenster','ruhebereich','airline','sparpreis','normalpreis','ändern','gangplatz','fensterplatz',
'flug','id','flughafen','bahnhof','zug','hbf','flugzeit','dauer','wagen','abfahrt']

#TODO more english words, reduce vocabulary_en_reduced afterwards
vocabulary_en=['fly','book','booking','cancel','rebook','cancellation','reserve','going',
'sorry','birthday','perso','ticket', 
'eco','hotel', 'airbnb','persons','bedrooms','find','close','far','distance','location',
'booked','nights','reservation','need','economy',
'business','class','premium','reasonable','sleep','miles','arrival','departure','timepoint','timerange','person',
'airline','passport','id','birthdate','birth','train','travel','go','duration','stops','fees','car','gate','terminal','cheapest','cheap']
vocabulary_en_reduced=['fly','book','booking','cancel','rebook','cancellation','reserve','going',
'sorry','birthday','perso','ticket','eco','hotel', 'airbnb','persons','bedrooms','distance','location','nights','reservation','economy',
'business','class','premium','reasonable','sleep','miles','arrival','departure','timepoint','timerange','person',
'airline','passport','id','birthdate','birth','train','travel','duration','stops','fees','car','gate','terminal','cheapest','cheap']


#count_vect = CountVectorizer(vocabulary=vocabulary)
texts_bag_words = []
for elem in texts:
	texts_bag_words.append(' '.join(str(w) for w in elem))


if language=="en":
 	    stop_words=stop_words_eng
elif language=="de":
        stop_words=stop_words_de	

y = np.array([1 if x == 'booking' else -1 for x in labels])
X = np.array([x for x in texts_bag_words])

#use weighting in traing
#sample_weight = np.array([4 if i == -1 else 1 for i in y])

n_outer_folds = 5
n_inner_folds = 10
skf_outer = StratifiedKFold(n_splits=n_outer_folds)
skf_outer_splits = skf_outer.split(X, y)

if language=="de":
    stemmer = GermanStemmer()
elif language=="en":    
    stemmer = EnglishStemmer()

analyzer = CountVectorizer().build_analyzer()

def stem_words(doc):
    return (stemmer.stem(w) for w in analyzer(doc))
def stem_vocab(vocab):
    return list(set([stemmer.stem(w) for w in vocab]))  
#vocabulary_de_en_stemmed = [stemmer.stem(w) for w in vocabulary_de_en]
vocabulary_de_en_stemmed=stem_vocab(vocabulary_de_en)
vocabulary_de_en_reduced_stemmed=stem_vocab(vocabulary_de_en_reduced)
vocabulary_de_stemmed=stem_vocab(vocabulary_de)
vocabulary_de_reduced_stemmed=stem_vocab(vocabulary_de_reduced)
vocabulary_en_stemmed=stem_vocab(vocabulary_en)
vocabulary_en_reduced_stemmed=stem_vocab(vocabulary_en_reduced)

#print(vocabulary_de_en_stemmed,"vocabulary_de_en_stemmed")
if language=="de":
		stemmed_vocabs=(vocabulary_de_en_stemmed,vocabulary_de_en_reduced_stemmed,vocabulary_de_stemmed,vocabulary_de_reduced_stemmed)
		unstemmed_vocabs=(vocabulary_de_en,vocabulary_de_en_reduced,vocabulary_de,vocabulary_de_reduced)
elif language=="en":
		stemmed_vocabs= (vocabulary_de_en_stemmed,vocabulary_de_en_reduced_stemmed,vocabulary_en_stemmed,vocabulary_en_reduced_stemmed)
		unstemmed_vocabs=(vocabulary_de_en,vocabulary_de_en_reduced,vocabulary_en,vocabulary_en_reduced)


print(stemmed_vocabs,"stemmed_vocabs")

# print(vocabulary_de_en_stemmed)
mean_tpr = 0.0
mean_fpr = np.linspace(0, 1, 100)


params = [{
	'union__tfidf__min_df': (1,20,40),
	'union__tfidf__sublinear_tf': (True,False),
	'union__tfidf__analyzer': ('word',stem_words),  
	# 'union__tfidf__max_features':(0.6,0.8,1.0),
	# 'union__tfidf__stop_words':(None, stop_words),
	'union__tfidf__ngram_range': ((1, 1),(1, 2),(1, 3)),

	'union__bow__vocabulary': unstemmed_vocabs,        
	'union__bow__analyzer': (['word']),

	'union__lda__pre__stop_words':(None, stop_words), #list of stop_words only possible for analyzer: word
	'union__lda__pre__analyzer': (['word']),
	'union__lda__post__n_topics': (5,10,50),
	'union__lda__post__max_iter': (10,50),
	'union__lda__post__learning_decay': (0.55,0.7,0.9),
	

	# 'clf__criterion': ('gini', 'entropy'),
	'clf__max_features':(1.0,0.7,0.3,0.1),
	# 'clf__n_estimators': (50, 100, 1000),
	'clf__max_depth': (3,8,None),
	'clf__min_samples_leaf': (1,5,10)
	}, 

	{
	'union__tfidf__min_df': (1,20,40),
	'union__tfidf__sublinear_tf': (True,False),
	'union__tfidf__analyzer': ('word',stem_words),  #stemming of vocabular list, perhaps not necessary because performed automatically with analyser
	# 'union__tfidf__max_features':(0.6,0.8,1.0),
	# 'union__tfidf__stop_words':(None, stop_words),
	'union__tfidf__ngram_range': ((1, 1),(1, 2),(1, 3)),

	'union__bow__vocabulary': stemmed_vocabs,
	'union__bow__analyzer': ([stem_words]),

	'union__lda__pre__stop_words':(None, stop_words),  #list of stop_words only possible for analyzer: word
	'union__lda__pre__analyzer': (['word']),
	'union__lda__post__n_topics': (5,10,50),
	'union__lda__post__max_iter': (10,50),
	'union__lda__post__learning_decay': (0.55, 0.7,0.9),

	# 'clf__criterion': ('gini', 'entropy'),
	'clf__max_features':(1.0,0.7,0.3,0.1),
	# 'clf__n_estimators': (50, 100, 1000),
	'clf__max_depth': (3,8,None),
	'clf__min_samples_leaf': (1,5,10)
	}, 

	{
	'union__tfidf__min_df': (1,20,40),
	'union__tfidf__sublinear_tf': (True,False),
	'union__tfidf__analyzer': ('word',stem_words),  
	# 'union__tfidf__max_features':(0.6,0.8,1.0),
	# 'union__tfidf__stop_words':(None, stop_words),
	'union__tfidf__ngram_range': ((1, 1),(1, 2),(1, 3)),

	'union__bow__vocabulary': unstemmed_vocabs,    
	'union__bow__analyzer': (['word']),

	'union__lda__pre__stop_words':([None]),
	'union__lda__pre__analyzer': ([stem_words]),   #list of stop_words not possible for analyzer stem_words
	'union__lda__post__n_topics': (5,10,50),
	'union__lda__post__max_iter': (10,50),
	'union__lda__post__learning_decay': (0.55, 0.7,0.9),

	# 'clf__criterion': ('gini', 'entropy'),
	'clf__max_features':(1.0,0.7,0.3,0.1),
	# 'clf__n_estimators': (50, 100, 1000),
	'clf__max_depth': (3,8,None),
	'clf__min_samples_leaf': (1,5,10)
	}, 

	{
	'union__tfidf__min_df': (1,20,40),
	'union__tfidf__sublinear_tf': (True,False),
	'union__tfidf__analyzer': ('word',stem_words),  
	# 'union__tfidf__max_features':(0.6,0.8,1.0),
	# 'union__tfidf__stop_words':(None, stop_words),
	'union__tfidf__ngram_range': ((1, 1),(1, 2),(1, 3)),

	'union__bow__vocabulary': stemmed_vocabs,     #stemming of vocabular list, perhaps not necessary because performed automatically with analyser
	'union__bow__analyzer': ([stem_words]),
  
	'union__lda__pre__stop_words':([None]),       #list of stop_words not possible for analyzer stem_words
	'union__lda__pre__analyzer': ([stem_words]),
	'union__lda__post__n_topics': (5,10,50),
	'union__lda__post__max_iter': (10,50),
	'union__lda__post__learning_decay': (0.55, 0.7,0.9),

	# 'clf__criterion': ('gini', 'entropy'),
	'clf__max_features':(1.0,0.7,0.3,0.1),
	# 'clf__n_estimators': (50, 100, 1000),
	'clf__max_depth': (3,8,None),
	'clf__min_samples_leaf': (1,5,10)
	}

    ]

best_params = []
for i, (tr_idx, te_idx) in enumerate(skf_outer_splits):
    rf_clf = Pipeline([
        ('union', FeatureUnion([
            ('tfidf', TfidfVectorizer(strip_accents='unicode', use_idf=1)),
            ('bow', CountVectorizer()),
            ('lda',Pipeline([('pre', CountVectorizer()),('post', LatentDirichletAllocation(n_jobs=-1,learning_method='online'))])),      
        ])),
        ('clf', RandomForestClassifier(n_jobs=-1,n_estimators=500))])
    # skf_inner = StratifiedKFold(n_splits=n_inner_folds)
    # print(skf_inner.get_n_splits(X[tr_idx],y[tr_idx]), "skf_inner.get_n_splits")   
    # cv = GridSearchCV(rf_clf, params, cv=skf_inner.get_n_splits(X[tr_idx],y[tr_idx]), n_jobs=1, verbose=1)
    cv = GridSearchCV(rf_clf, params, cv=n_inner_folds, n_jobs=1, verbose=1)  #if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used.
    res = cv.fit(X[tr_idx], y[tr_idx])
    proba = res.best_estimator_.predict_proba(X[te_idx])
    best_params.append(cv.best_params_)
    fpr, tpr, ths = metrics.roc_curve(y[te_idx], proba[:, 1])
    mean_tpr += interp(mean_fpr, fpr, tpr)
    mean_tpr[0] = 0.0
    roc_auc = metrics.auc(fpr, tpr)
    print("...fold {} roc auc {:.2f}".format(i, roc_auc))
    

mean_tpr /= n_outer_folds
mean_roc_auc = metrics.auc(mean_fpr, mean_tpr)

print("Finished: avg roc auc {:.2f}".format(mean_roc_auc))
print(cv.best_estimator_, best_params, mean_roc_auc)
fig = figure(title='roc analysis random forest', x_axis_label='fp', y_axis_label='tp')
fig.line(mean_fpr, mean_tpr, line_width=3, line_color='red',legend='ROC avg (area={:.2f})'.format(mean_roc_auc))
show(fig)


# # needed for pipeline with GradientBoostingClassifier that accepts only dense matrices as input
# class DenseTransformer(BaseEstimator):

#     def transform(self, X, y=None, **fit_params):
#         return X.todense()

#     def fit_transform(self, X, y=None, **fit_params):
#         self.fit(X, y, **fit_params)
#         return self.transform(X)

#     def fit(self, X, y=None, **fit_params):
#         return self

 
# best_params = []
# for i, (tr_idx, te_idx) in enumerate(skf_outer_splits):
#     gb_clf = Pipeline([
#         ('union', FeatureUnion([
#             ('tfidf', TfidfVectorizer(strip_accents='unicode',analyzer='word')),
#             ('bow', CountVectorizer(vocabulary=vocabulary)),
#             ('lda',Pipeline([('pre', CountVectorizer()),('post', LatentDirichletAllocation(n_jobs=1,learning_method='online'))])),      
#         ])),
#         ('dense', DenseTransformer()), 
#         ('clf', GradientBoostingClassifier())])
#     # inner_cv = StratifiedKFold(y[tr_idx], n_splits=n_inner_folds)
#     cv = GridSearchCV(rf_clf, params, cv=n_inner_folds, n_jobs=1, verbose=1)  #if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used.
#     res = cv.fit(X[tr_idx], y[tr_idx])
#     proba = res.best_estimator_.predict_proba(X[te_idx])
#     best_params.append(cv.best_params_)
#     fpr, tpr, ths = metrics.roc_curve(y[te_idx], proba[:, 1])
#     mean_tpr += interp(mean_fpr, fpr, tpr)
#     mean_tpr[0] = 0.0
#     roc_auc = metrics.auc(fpr, tpr)
#     print("...fold {} roc auc {:.2f}".format(i, roc_auc))
   
# mean_tpr /= n_outer_folds
# mean_roc_auc = metrics.auc(mean_fpr, mean_tpr)
# print("Finished: avg roc auc {:.2f}".format(mean_roc_auc))
# print(cv.best_estimator_, best_params, mean_roc_auc)
# fig = figure(title='roc analysis SVM', x_axis_label='fp', y_axis_label='tp')
# fig.line(mean_fpr, mean_tpr, line_width=3, line_color='red',legend='ROC avg (area={:.2f})'.format(mean_roc_auc))
# show(fig)


# mean_tpr = 0.0
# mean_fpr = np.linspace(0, 1, 100)
# # C=[2**(-5), 2**(-3), 2**(-1), 2, 2**4, 2**6, 2**8, 2**10, 2**12, 2**14]
# # Gamma = [2**(-15), 2**(-13), 2**(-11), 2**(-9), 2**(-7), 2**(-5), 2**(-3), 2**(-1), 2, 2**2]
# params = [ {'union__tfidf__min_df': (1, 10),
#      #'union__tfid__sublinear_tf: (True, False),
#     # 'union__tfidf__stop_words':(None, stop_words),
#     # 'union__bow__ngram_range': ((1, 1), (1, 2), (1, 3)),
#     # 'union__lda__pre__stop_words':(None, stop_words), 
#     # 'union__lda__post__n_topics': (5,10,50),
#     # # 'union__lda__post__max_iter': (10,50),
#     'clf__kernel': (["poly"]),
#     'clf__degree': (2,3,5),
#     'clf__C': (np.logspace(-3, 2, 6)),
#     'clf__gamma': (np.logspace(-3, 2, 6))
#     }, 
#     {
#     # 'union__tfidf__min_df': (1, 10),
#     # 'union__tfidf__stop_words':(None, stop_words),
#     # 'union__bow__ngram_range': ((1, 1), (1, 2), (1, 3)),
#     # 'union__lda__pre__stop_words':(None, stop_words), 
#     # 'union__lda__post__n_topics': (5,10,50),
#     # 'union__lda__post__max_iter': (10,50),
#     'clf__kernel': (["rbf"]),
#     'clf__C': (np.logspace(-3, 2, 6)),
#     'clf__gamma': (np.logspace(-3, 2, 6))
#     },
#     {
#     # 'union__tfidf__min_df': (1, 10),
#     # 'union__tfidf__stop_words':(None, stop_words),
#     # 'union__bow__ngram_range': ((1, 1), (1, 2), (1, 3)),
#     # 'union__lda__pre__stop_words':(None, stop_words), 
#     # 'union__lda__post__n_topics': (5,10,50),
#     # 'union__lda__post__max_iter': (10,50),
#     'clf__kernel': (["linear"])
#     # 'clf__C': (np.logspace(-3, 2, 6)) 
#     }
#     ]

     
# best_params = []
# for i, (tr_idx, te_idx) in enumerate(skf_outer_splits):
#     svm_clf = Pipeline([
#         ('union', FeatureUnion([
#             ('tfidf', TfidfVectorizer(max_features=None, strip_accents='unicode',  
#        analyzer='word')),
#             ('bow', CountVectorizer(vocabulary=vocabulary)),
#             ('lda',Pipeline([('pre', CountVectorizer()),('post', LatentDirichletAllocation(learning_method='online'))])),      
#         ])),
#          ('norm', Normalizer(norm='l2')),
#         ('clf', SVC(probability=True))])
#     # inner_cv = StratifiedKFold(y[tr_idx], n_splits=n_inner_folds)
#     cv = GridSearchCV(rf_clf, params, cv=n_inner_folds, n_jobs=1, verbose=1)  #if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used.
#     res = cv.fit(X[tr_idx], y[tr_idx])
#     proba = res.best_estimator_.predict_proba(X[te_idx])
#     best_params.append(cv.best_params_)
#     fpr, tpr, ths = metrics.roc_curve(y[te_idx], proba[:, 1])
#     mean_tpr += interp(mean_fpr, fpr, tpr)
#     mean_tpr[0] = 0.0
#     roc_auc = metrics.auc(fpr, tpr)
#     print("...fold {} roc auc {:.2f}".format(i, roc_auc))
   
# mean_tpr /= n_outer_folds
# mean_roc_auc = metrics.auc(mean_fpr, mean_tpr)
# print("Finished: avg roc auc {:.2f}".format(mean_roc_auc))
# print(cv.best_estimator_, best_params, mean_roc_auc)
# fig = figure(title='roc analysis SVM', x_axis_label='fp', y_axis_label='tp')
# fig.line(mean_fpr, mean_tpr, line_width=3, line_color='red',legend='ROC avg (area={:.2f})'.format(mean_roc_auc))
# show(fig)


# mean_tpr = 0.0
# mean_fpr = np.linspace(0, 1, 100)

# params = {
#     'union__tfidf__min_df': (1, 10),
#     #'union__tfidf__stop_words':(None, stop_words),
#     #'union__tfid__sublinear_tf: (True, False),
#     # 'union__bow__ngram_range': ((1, 1), (1, 2), (1, 3)),
#     #'union__lda__pre__stop_words':(None, stop_words), 
#     # 'union__lda__post__n_topics': (5,10,50),
#     # 'union__lda__post__max_iter': (10,50),
#     #'clf__n_estimators': (10, 100, 500)
#     #'clf__max_depth': (3, 5, 8),
#     # 'clf__learning_rate': (0.01, 0.05, 0.1, 0.15),
#     # 'clf__subsample': (0.7, 0.85, 1),
#     # 'clf__min_samples_leaf': (1, 5)
#     #'clf__n_estimators': (50, 100, 300)
#     'clf__max_depth': (3,5)
#     }
     



# mean_tpr = 0.0
# mean_fpr = np.linspace(0, 1, 100)
# params = {
#     'union__tfidf__min_df': (1, 5, 10),
#     # 'union__tfidf__stop_words':(None, stop_words),
#     'union__bow__ngram_range': ((1, 1), (1, 2), (1, 3)),
#     'union__lda__postlda__n_topics': (10,50),
#     # 'union__lda__postlda__max_iter': (10,50),
#     'clf__max_depth': (3,5)} 

# best_params = []
# for i, (tr_idx, te_idx) in enumerate(skf_outer_splits):
#     rf_clf = Pipeline([
#         ('union', FeatureUnion([
#             ('tfidf', TfidfVectorizer(max_features=None, strip_accents='unicode',  
#        analyzer='word',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1, stop_words=stop_words)),
#             ('bow', CountVectorizer(vocabulary=vocabulary)),
#             ('lda',Pipeline([('pre', CountVectorizer()),('post', LatentDirichletAllocation(learning_method='online'))])),      
#         ])),
#         ('clf', RandomForestClassifier())])
#     #inner_cv = StratifiedKFold(y[tr_idx], n_splits=n_inner_folds)
#     cv = GridSearchCV(rf_clf, params, cv=n_inner_folds, n_jobs=1, verbose=1)  #if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used.
#     res = cv.fit(X[tr_idx], y[tr_idx])
#     proba = res.best_estimator_.predict_proba(X[te_idx])
#     best_params.append(cv.best_params_)
#     fpr, tpr, ths = metrics.roc_curve(y[te_idx], proba[:, 1])
#     mean_tpr += interp(mean_fpr, fpr, tpr)
#     mean_tpr[0] = 0.0
#     roc_auc = metrics.auc(fpr, tpr)
#     print("...fold {} roc auc {:.2f}".format(i, roc_auc))
    

# mean_tpr /= n_outer_folds
# mean_roc_auc = metrics.auc(mean_fpr, mean_tpr)
# print("Finished: avg roc auc {:.2f}".format(mean_roc_auc))
# print(cv.best_estimator_, best_params, mean_roc_auc)


# mean_tpr = 0.0
# mean_fpr = np.linspace(0, 1, 100)
# params = {
#     'union__tfidf__min_df': (1, 5, 10),
#     'union__tfidf__stop_words':(None, stop_words),
#     'union__bow__ngram_range': ((1, 1), (1, 2), (1, 3)),
#     'union__lda__n_topics': (10,20,50,100),
#     'union__lda__max_iter': (10,50),
#     'clf__alpha': (0.1, 0.5, 1.0)}


# best_params = []
# for i, (tr_idx, te_idx) in enumerate(skf_outer_splits):
#     gnb_clf = Pipeline([
#         ('union', FeatureUnion([
#             ('tfidf', TfidfVectorizer(max_features=None, strip_accents='unicode',  
#        analyzer='word',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1, stop_words=stop_words)),
#             ('bow', CountVectorizer(vocabulary=vocabulary)),
#             ('lda', LatentDirichletAllocation(learning_method='online'))      
#         ])),
#         ('clf', MultinomialNB())])
#     #inner_cv = StratifiedKFold(y[tr_idx], n_splits=n_inner_folds)
#     cv = GridSearchCV(rf_clf, params, cv=n_inner_folds, n_jobs=1, verbose=1)  #if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used.
#     res = cv.fit(X[tr_idx], y[tr_idx])
#     proba = res.best_estimator_.predict_proba(X[te_idx])
#     best_params.append(cv.best_params_)
#     fpr, tpr, ths = metrics.roc_curve(y[te_idx], proba[:, 1])
#     mean_tpr += interp(mean_fpr, fpr, tpr)
#     mean_tpr[0] = 0.0
#     roc_auc = metrics.auc(fpr, tpr)
#     print("...fold {} roc auc {:.2f}".format(i, roc_auc))
    

# mean_tpr /= n_outer_folds
# mean_roc_auc = metrics.auc(mean_fpr, mean_tpr)
# print("Finished: avg roc auc {:.2f}".format(mean_roc_auc))
# print(cv.best_estimator_, best_params, mean_roc_auc)

# mean_tpr = 0.0
# mean_fpr = np.linspace(0, 1, 100)
# params = {
#     'union__tfidf__min_df': (1, 10),
#     'union__tfidf__stop_words':(None, stop_words),
#     'union__bow__ngram_range': ((1, 1), (1, 2), (1, 3)),
#     'clf__alpha': (0.1, 0.5, 1.0)}

 
# best_params = []
# for i, (tr_idx, te_idx) in enumerate(skf_outer_splits):
#     gnb_clf = Pipeline([
#         ('union', FeatureUnion([
#             ('tfidf', TfidfVectorizer(max_features=None, strip_accents='unicode',  
#        analyzer='word',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1, stop_words=stop_words)),
#             ('bow', CountVectorizer(vocabulary=vocabulary))
#         ])),
#         ('clf', MultinomialNB())])
#     #inner_cv = StratifiedKFold(y[tr_idx], n_splits=n_inner_folds)
#     cv = GridSearchCV(rf_clf, params, cv=n_inner_folds, n_jobs=1, verbose=1)  #if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used.
#     res = cv.fit(X[tr_idx], y[tr_idx])
#     proba = res.best_estimator_.predict_proba(X[te_idx])
#     best_params.append(cv.best_params_)
#     fpr, tpr, ths = metrics.roc_curve(y[te_idx], proba[:, 1])
#     mean_tpr += interp(mean_fpr, fpr, tpr)
#     mean_tpr[0] = 0.0
#     roc_auc = metrics.auc(fpr, tpr)
#     print("...fold {} roc auc {:.2f}".format(i, roc_auc))
    
# mean_roc_auc = metrics.auc(mean_fpr, mean_tpr)
# print("Finished: avg roc auc {:.2f}".format(mean_roc_auc))
# print(cv.best_estimator_, best_params, mean_roc_auc)



# mean_tpr = 0.0
# mean_fpr = np.linspace(0, 1, 100)
# params = {
#     'union__tfidf__min_df': (1, 10),
#     'union__bow__ngram_range': ((1, 1), (1, 2), (1, 3)),
#     'clf__max_depth': (3, 5, 10)}

# best_params = []
# for i, (tr_idx, te_idx) in enumerate(skf_outer_splits):
#     rf_clf = Pipeline([
#         ('union', FeatureUnion([
#             ('tfidf', TfidfVectorizer(max_features=None, strip_accents='unicode',  
#        analyzer='word',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1, stop_words=stop_words)),
#             ('bow', CountVectorizer(vocabulary=vocabulary))
#         ])),
#         ('clf', RandomForestClassifier())])
#     #inner_cv = StratifiedKFold(y[tr_idx], n_splits=n_inner_folds)
#     cv = GridSearchCV(rf_clf, params, cv=n_inner_folds, n_jobs=1, verbose=1)  #if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used.
#     res = cv.fit(X[tr_idx], y[tr_idx])
#     proba = res.best_estimator_.predict_proba(X[te_idx])
#     best_params.append(cv.best_params_)
#     fpr, tpr, ths = metrics.roc_curve(y[te_idx], proba[:, 1])
#     mean_tpr += interp(mean_fpr, fpr, tpr)
#     mean_tpr[0] = 0.0
#     roc_auc = metrics.auc(fpr, tpr)
#     print("...fold {} roc auc {:.2f}".format(i, roc_auc))
   

# mean_tpr /= n_outer_folds
# mean_roc_auc = metrics.auc(mean_fpr, mean_tpr)

# print("Finished: avg roc auc {:.2f}".format(mean_roc_auc))
# print(cv.best_estimator_, best_params, mean_roc_auc)