# -*- coding: utf-8 -*-
from __future__ import absolute_import
import random
from collections import defaultdict
import logging
from perceptron import AveragedPerceptron

logger = logging.getLogger(__name__)


class PerceptronTagger():

    '''Greedy Averaged Perceptron tagger, as implemented by Matthew Honnibal.

    See more implementation details here:
        http://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/

    :param load: Load the pickled model upon instantiation.
    '''

    START = ['-START-', '-START2-']
    END = ['-END-', '-END2-']

    def __init__(self):
        self.model = AveragedPerceptron()
        self.tagdict = {}
        self.classes = set()

    def tag(self, token_seqs, tag_name='tag'):
        '''Tags a sequence of tokens'''
        def tag_one(tokens):
            prev, prev2 = self.START
            for i, t in enumerate(tokens):
                word = self._normalize(t)
                tag = self.tagdict.get(word)
                if not tag:
                    features = self._get_features(i, tokens, prev, prev2)
                    tag = self.model.predict(features)
                t[tag_name] = tag
                prev2 = prev
                prev = tag

        if type(token_seqs[0]) is dict:
            tag_one(token_seqs)
        else:
            for tokens in token_seqs:
                tag_one(tokens)

    def gen_dataset(self, token_seqs, label_key='label'):
        prev, prev2 = self.START
        X = []
        y = []
        for tokens in token_seqs:
            for i, token in enumerate(tokens):
                X.append(self._get_features(i, tokens, prev, prev2))
                y.append(token[label_key])
        return X, y

    def train(self, token_seqs, val_token_seqs, nr_iter=5, label_key='label'):
        '''Train a model from sequences of tokens. ``nr_iter``
        controls the number of Perceptron training iterations.
        '''
        self._make_tagdict(token_seqs, label_key)
        self.model.classes = self.classes
        prev, prev2 = self.START
        for iter_ in range(nr_iter):
            random.shuffle(token_seqs)
            c = n = 0
            c_notO = n_notO = 0
            for tokens in token_seqs:
                for i, token in enumerate(tokens):
                    guess = self.tagdict.get(self._normalize(token))
                    if not guess:
                        feats = self._get_features(i, tokens, prev, prev2)
                        guess = self.model.predict(feats)
                        self.model.update(token[label_key], guess, feats)
                    prev2 = prev
                    prev = guess
                    c += guess == token[label_key]
                    n += 1
                    if token[label_key] != 'O':
                        c_notO += guess == token[label_key]
                        n_notO += 1
            cv = nv = 0
            cv_notO = nv_notO = 0
            for tokens in val_token_seqs:
                for i, token in enumerate(tokens):
                    guess = self.tagdict.get(self._normalize(token))
                    if not guess:
                        feats = self._get_features(i, tokens, prev, prev2)
                        guess = self.model.predict(feats)
                    prev2 = prev
                    prev = guess
                    cv += guess == token[label_key]
                    nv += 1
                    if token[label_key] != 'O':
                        cv_notO += guess == token[label_key]
                        nv_notO += 1
            logging.info("Iter {:2d}: all {}/{}={:.1%} not O: {}/{}={:.1%}".format(
                iter_, c, n, c/n, c_notO, n_notO, c_notO/n_notO))
            logging.info("            val {}/{}={:.1%} not O: {}/{}={:.1%}".format(
                cv, nv, cv/nv, cv_notO, nv_notO, cv_notO/nv_notO))
        self.model.average_weights()

    def _normalize(self, token):
        '''Normalization used in pre-processing.

        - All words are lower cased
        - Digits in the range 1800-2100 are represented as !YEAR;
        - Other digits are represented as !DIGITS
        '''
        if 'SPACE' in token['pos']:
            # tokens identified as B/I-SPACE by spacy are returned as
            # such
            return token['pos']
        if 'PUNCT' in token['pos']:
            # tokens identified as B/I-PUNCT by spacy are returned as
            # such
            return token['pos']
        word = token['text']
        return word.lower()

    def _get_tok(self, i, tokens):
        if i < 0:
            word = 'START_{}'.format(i)
            token = {'shape': 'START', 'rner': 'O-'}
        elif i >= len(tokens):
            word = 'END_{}'.format(i-len(tokens))
            token = {'shape': 'END', 'rner': 'O-'}
        else:
            word = self._normalize(tokens[i])
            token = tokens[i]
        return word, token

    def _get_features(self, i, tokens, prev, prev2):
        '''Map tokens into a feature representation, implemented as a
        {hashable: float} dict. If the features change, a new model must be
        trained.
        '''
        def add(name, *args):
            features[' '.join((name,) + tuple(args))] += 1

        features = defaultdict(int)
        word_i, token_i = self._get_tok(i, tokens)
        word_i_p1, token_i_p1 = self._get_tok(i-1, tokens)
        word_i_p2, token_i_p2 = self._get_tok(i-2, tokens)
        word_i_n1, token_i_n1 = self._get_tok(i+1, tokens)
        word_i_n2, token_i_n2 = self._get_tok(i+2, tokens)
        # It's useful to have a constant feature, which acts sort of like a prior
        add('bias where', token_i['where'])
        add('i-1 tag', prev)
        add('i-2 tag', prev2)
        add('i tag+i-2 tag', prev, prev2)

        add('i word', word_i)
        add('i shape', token_i['shape'])
        add('i rner', token_i['rner'])
        add('i suffix 3', word_i[-3:])
        add('i prefix 1', word_i[0])

        add('i-1 tag+i word', prev, word_i)

        add('i-1 word', word_i_p1)
        add('i-1 shape', token_i_p1['shape'])
        add('i-1 rner', token_i_p1['rner'])
        add('i-1 suffix', word_i_p1[-3:])
        add('i-1 rner', token_i_p1['rner'])
        add('i-2 word', word_i_p2)

        add('i+1 word', word_i_n1)
        add('i+1 shape', token_i_n1['shape'])
        add('i+1 rner', token_i_n1['rner'])
        add('i+1 suffix', word_i_n1[-3:])
        add('i+2 word', word_i_n2)
        return features

    def _make_tagdict(self, token_seqs, label_key):
        '''Make a tag dictionary for single-tag words.

        Store words which occur at least 20 times in the corpus and
        are in 97%+ of the cases assigned to one tag.
        '''
        counts = defaultdict(lambda: defaultdict(int))
        for tokens in token_seqs:
            for token in tokens:
                counts[self._normalize(token)][token[label_key]] += 1
                self.classes.add(token[label_key])
        freq_thresh = 50
        ambiguity_thresh = 0.97
        for word, tag_freqs in counts.items():
            tag, mode = max(tag_freqs.items(), key=lambda item: item[1])
            n = sum(tag_freqs.values())
            # Don't add rare words to the tag dictionary
            # Only add quite unambiguous words
            if n >= freq_thresh and (float(mode) / n) >= ambiguity_thresh:
                self.tagdict[word] = tag
        logger.info('tag dict contains {}Â entries'.format(len(self.tagdict)))
