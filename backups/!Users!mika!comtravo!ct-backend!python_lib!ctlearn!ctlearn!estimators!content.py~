from ..base_model import BaseModel
from . content_model_helpers import ann2txt, best_of_folds
import numpy as np
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.model_selection import GridSearchCV, KFold
from scipy import interp
from sklearn.metrics import roc_curve, auc
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer
import os


class ContentModel(BaseModel):
    def __init__(self):
        self.model = None
        self.model_name = "content_model"
        self.model_path = os.path.dirname(__file__) + "/models/" + self.model_name + ".pkl"

    def fit(self, X, y):
        self.model = {}
        for lang in ["de", "en"]:
            X_ = X[lang]
            y_ = y[lang]
            mlb = MultiLabelBinarizer()
            y_ = mlb.fit_transform(y_)
            rf_clf = Pipeline([
                ('union', FeatureUnion([
                    ('tfidf', TfidfVectorizer(strip_accents='unicode', use_idf=1,
                                              ngram_range=(1, 1)))
                ])),
                ('clf', RandomForestClassifier(n_jobs=-1, n_estimators=100, min_samples_leaf=1))])
            rf_clf._set_params('steps', **self.params[lang])
            rf_clf.fit([ann2txt(d["annotations"]) for d in X_], y_)
            self.model[lang] = {"clf": rf_clf, "mlb_classes": mlb.classes_}

    def apply(self, sample, lang=None):
        if self.model:
            if not lang:
                lang = sample['language']
            if lang in self.model.keys():
                p = self.model[lang]["clf"].predict_proba([ann2txt(sample['annotations'])])
                content_probs = {}
                for idx, elem in enumerate(p):
                    content_probs[self.model[lang]["mlb_classes"][idx]] = p[idx][0][1]
                return content_probs
            else:
                return {}
        else:
            raise TypeError('No model loaded!')

    @classmethod
    def prepare_data(cls, data):
        '''
        This method implements the model specific preprocessing function.
        Should return:
            - X: iterable/dict of iterables
            - y: iterable/dict of iterables
        '''
        X_dict, y_dict = {}, {}
        for lang in ["de", "en"]:
            messages = []
            for elem in data:
                msg = {'language': elem['ann']['language'],
                       'labels': [el['type'] for el in elem['ann']['events']],
                       'annotations': elem['ann']['annotations']}
                messages.append(msg)

            X = [message for message in messages
                 if message['language'] == lang]
            if lang == 'de':
                stopwords = []
            elif lang == 'en':
                stopwords = ['RentalCar', 'Transfer']
            y = [list(set([w.replace('ReturnFlight', 'Flight') for w in message['labels']
                           if w not in stopwords])) if message['labels'] else ['Other']
                 for message in messages if message['language'] == lang]
            # Map the labels to what we want to have in the predictions (the
            # format in which the nlp service will hand the result out)
            map_labels = {
                'Flight': 'flight',
                'RentalCar': 'rental_car',
                'Transfer': 'transfer',
                'Hotel': 'hotel',
                'Train': 'train',
                'Other': 'other',
                '': 'other'
            }
            y = [[map_labels[e] for e in elem] for elem in y]
            X_dict[lang] = X
            y_dict[lang] = y
        return X_dict, y_dict

    def metric(self, y_true, y_pred):
        score, cnt = 0, 0
        for t, p in zip(y_true, y_pred):
            if len(t) > 0:
                if t[0] == max(p, key=p.get):
                    score += 1
            cnt += 1
        return score/cnt

    def score(self, X, y):
        '''
        Validates the model.
        Input:
            - data: list, same length as labels
            - labels: list or array, same length as data
            - metric: callable taking labels and
                      predictions of the model as input
        '''
        if self.model:
            preds = []
            labels = []
            for lang in ["de", "en"]:
                for s, l in zip(X[lang], y[lang]):
                    p = self.apply(s, lang)
                    preds.append(p)
                    labels.append(l)
            return self.metric(np.array(labels), np.array(preds))
        else:
            raise ValueError

    def tune(self, X, y):
        '''
        Do a grid search CV with the model and return the best parameters.
        '''
        self.params = {}
        for lang in ["de", "en"]:
            X_ = X[lang]
            y_ = y[lang]
            mlb = MultiLabelBinarizer()
            y_ = mlb.fit_transform(y_)
            number_classes = y_.shape[1]
            n_outer_folds = 3
            n_inner_folds = 5
            skf_outer = KFold(n_splits=n_outer_folds)
            skf_outer_splits = skf_outer.split(X_, y_)
            mean_tpr = 0.0
            mean_fpr = np.linspace(0, 1, 100)
            mean_tpr_classes = [0.0]*number_classes
            params = {
                'union__tfidf__ngram_range': [(1, 1), (1, 2)],
                'clf__min_samples_leaf': [1, 3],
                'clf__n_estimators': [1000]
            }

            best_params = []
            for i, (tr_idx, te_idx) in enumerate(skf_outer_splits):

                rf_clf = Pipeline([
                    ('union', FeatureUnion([
                        ('tfidf', TfidfVectorizer(strip_accents='unicode', use_idf=1))
                    ])),
                    ('clf', RandomForestClassifier(n_jobs=-1))])
                cv = GridSearchCV(rf_clf, params, cv=n_inner_folds, n_jobs=1, verbose=0)
                cv.fit([ann2txt(X_[i]["annotations"]) for i in tr_idx], y_[tr_idx])
                proba = cv.best_estimator_.predict_proba([ann2txt(X_[i]["annotations"])
                                                          for i in te_idx])
                # prob = np.asarray(proba)[:, :, 1].T
                # if lang == 'de':
                #     thresholds = [0.45, 0.4, 0.4, 0.15, 0.2, 0.1]
                # elif lang == 'en':
                #     thresholds = [0.45, 0.3, 0.5, 0.2]
                # pred_classes = np.greater(prob, thresholds)
                best_params.append(cv.best_params_)

                fpr = dict()
                tpr = dict()
                ths = dict()
                roc_auc = []
                for j in range(y_[te_idx].shape[1]):
                    fpr[j], tpr[j], ths[j] = roc_curve(y_[te_idx, j], proba[j][:, 1])
                    mean_tpr_classes[j] += interp(mean_fpr, fpr[j], tpr[j])
                    mean_tpr_classes[j][0] = 0
                    roc_auc_class = auc(fpr[j], tpr[j])
                    roc_auc.append(roc_auc_class)
                    mean_tpr += interp(mean_fpr, fpr[j], tpr[j])
                # roc_auc_avg = np.average(roc_auc)
                mean_tpr[0] = 0.0
                # print("...fold {} roc auc {:.2f}".format(i, roc_auc_avg))
                # some_metrics = classification_report(y[te_idx], pred_classes)
                # print(some_metrics, mlb.classes_)
            mean_tpr_classes = [mean_tpr_class/n_outer_folds for mean_tpr_class in mean_tpr_classes]
            mean_tpr /= (n_outer_folds*number_classes)
            mean_roc_auc = auc(mean_fpr, mean_tpr)

            print("Finished: avg roc auc {:.2f}".format(mean_roc_auc))
            selected_params = best_of_folds(best_params)
            print(selected_params)
            self.params[lang] = selected_params
