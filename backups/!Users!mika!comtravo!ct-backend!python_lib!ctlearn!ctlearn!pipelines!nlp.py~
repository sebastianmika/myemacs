import numpy as np
from dask.optimize import cull
from dask.async import get_sync
from dask.base import visualize
from ..base import Transformer, Estimator
from ..estimators import TestEstimator
from ..transformations import TestTransformer, LanguageTransformer, SourceTransformer,\
   TypeTransformer, EOMTransformer, GreetingTransformer, ReplyTransformer,\
   TruncateTransformer, IATATransformer, CRMCompanyTransformer,\
   CRMPersonTransformer, PersonTransformer, TimeTransformer,\
   FlightNumberTransformer, LocationTransformer, AirlineTransformer,\
   CategoryOrClassTransformer, LuggageTransformer, CompanyProbTransformer,\
   PersonProbTransformer, SpacyTransformer, MergeTransformer


class NLPPipeline(Estimator):
    def __init__(self, get_fun=get_sync):
        super(NLPPipeline, self).__init__(input_keys='X', output_key='res')
        self.get_fun = get_fun
        self._setup()

    def _setup(self):
        self.pipeline = []
        self.pipeline.append(SourceTransformer())
        self.pipeline.append(TypeTransformer())
        self.pipeline.append(EOMTransformer())
        self.pipeline.append(ReplyTransformer())
        self.pipeline.append(GreetingTransformer())
        self.pipeline.append(LanguageTransformer())
        self.pipeline.append(TruncateTransformer())
        self.pipeline.append(CRMCompanyTransformer())
        self.pipeline.append(CRMPersonTransformer())
        self.pipeline.append(PersonTransformer())
        self.pipeline.append(TimeTransformer())
        self.pipeline.append(SpacyTransformer())
        self.pipeline.append(IATATransformer())
        self.pipeline.append(FlightNumberTransformer())
        self.pipeline.append(LocationTransformer())
        self.pipeline.append(AirlineTransformer())
        self.pipeline.append(CategoryOrClassTransformer())
        self.pipeline.append(LuggageTransformer())
        self.pipeline.append(CompanyProbTransformer())
        self.pipeline.append(PersonProbTransformer())
        self.pipeline.append(MergeTransformer())
        self.pipeline.append(TestEstimator('p_book', ['annotation']))
        self.pipeline.append(TestEstimator('content', ['annotation']))
        self.pipeline.append(TestEstimator('event', ['annotation', 'language']))
        self.pipeline.append(TestEstimator('confidence', ['event']))

    def _transform_dsk(self, X):
        '''Create a 'transform' DAG by adding all 'output_key = transform(X,
        *input_keys)' pairs to the graph and setting the initial 'msg' to X.
        '''
        dsk = {
            p.output_key: (p.transform, *p.input_keys) for p in self.pipeline
        }
        dsk['msg'] = X
        return dsk

    @classmethod
    def _join_split(cls, *args):
        return [x for split in args for x in split]

    @classmethod
    def _make_split(cls, nfold, one_x):
        N = len(one_x)
        K = N // nfold
        perm = list(np.random.permutation(N))
        perm = list(range(N))
        splits = []
        for i in range(nfold):
            off = 1 if i == nfold - 1 else 0
            splits.append(
                {'train_idx': perm[:(i*K)] + perm[(i+1)*K+off:],
                 'test_idx': perm[i*K:(i+1)*K+off]})
        return splits

    def _fit_dsk(self, X, y):
        '''Fit a complete pipeline the proper way (not the sklearn way of just
        fitting on the input data and then passing on the transform of just
        that data):
        
        - Only pipeline elements with are Estimators need to be fitted,
          for all other we call transform
        
        - For Estimators that *do not have* children,
          call fit on X, y and return None - there shouldn't be any
          children waiting for the output

        - For pipeline elements with fit that *do have* children, we
          first fit 'n_fit_fold' models on corresponding cv splits and
          generate an unbiased prediction which we forward and do a
          final fit on X, y
        '''
        dsk = {}
        for p in self.pipeline:
            if isinstance(p, Estimator):
                # Fit a model on all data for future use
                dsk[(p.name, 'fitted_model')] = (p.fit, *p.input_keys)
                # Split X, y into n_fit_fold chunks as stored in p to
                # generate predictions for downstream elements if any

                # ToDo: check if we are not the last node in the graph
                n_fit_fold = 10
                dsk[(p.name, 'kfold')] = (
                    NLPPipeline._make_split,
                    n_fit_fold, p.input_keys[0])
                for i in range(n_fit_fold):
                    # 1/n_fit_fold of the data
                    fold_p = p.clone()
                    dsk[(p.name, 'fit_transform', i)] = (
                        fold_p.fit_transform,
                        (p.name, 'kfold'), i, *p.input_keys)
                dsk[p.output_key] = (
                    NLPPipeline._join_split,
                    *[(p.name, 'fit_transform', i)
                      for i in range(n_fit_fold)])
            elif isinstance(p, Transformer):
                # a Transformer - just call transform on the required
                # inputs and generate the output
                dsk[p.output_key] = (p.transform, *p.input_keys)
            else:
                ValueError('unknown type {} in pipeline'.format(type(p)))
        dsk['msg'] = X
        return dsk
        
    def transform(self, X):
        return lambda x: self.get_fun(self._transform_dsk(X), x)

    def fit(self, X, y):
        for p in self.pipeline:
            if isinstance(p, Estimator):
                # insert a fit artifact with n-folds forward fitting
                self.dsk_fit_pipeline[p.output_key] = (p.fit, *p.input_keys)
            elif isinstance(p, Transformer):
                self.dsk_fit_pipeline[p.output_key] = (p.transform, *p.input_keys)
            else:
                raise ValueError('unknown type in pipeline fit')

    @classmethod
    def test(cls):
        pipe = cls()
        X = [{'msg_id': i, 'body': 'This is some random English test, please detect me',
              'subject': 'And here is the subject'} for i in range(10)]
        dsk = pipe._transform_dsk(X)
        dsk_opt, deps = cull(dsk, ['p_book', 'event', 'confidence', 'prob', 'content'])
        visualize(dsk, optimize_graph=False, filename='nlp_dsk.png', rankdir='LR')
        visualize(dsk_opt, optimize_graph=True, filename='nlp_opt_dsk.png', rankdir='LR')
        return pipe.transform(X), pipe

        

"""
dsk_opt, deps = cull(dsk, ['p_book', 'event', 'confidence', 'prob', 'content'])
visualize(dsk_opt, optimize_graph=True, filename='nlp_dsk.png', rankdir='LR')
get_sync(dsk, 'event')
"""
