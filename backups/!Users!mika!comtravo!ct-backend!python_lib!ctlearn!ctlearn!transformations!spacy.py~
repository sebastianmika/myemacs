from .. base import Transformer
import spacy
import spacy.about as about
import sputnik
import logging
import regex
from sputnik.package_list import PackageNotFoundException, CompatiblePackageNotFoundException


logger = logging.getLogger(__name__)


def _is_spacy_model_installed(lang):
    try:
        sputnik.package(about.__title__, about.__version__, about.__models__[lang])
        return True
    except (PackageNotFoundException, CompatiblePackageNotFoundException):
        return False


def _spacy_model_install(lang):
    logger.info('Installing spacy models for lang={}... hold on'.format(lang))
    sputnik.install(about.__title__, about.__version__, about.__models__[lang])
    try:
        sputnik.package(about.__title__, about.__version__, about.__models__[lang])
    except (PackageNotFoundException, CompatiblePackageNotFoundException):
        logger.error("Spacy model for lang = {} failed to install".format(lang))
        raise Exception("Installing spacy model for lang={} failed".format(lang))


def _spacy_model_load(lang):
    logger.info('Loading spacy model for lang={}...'.format(lang))
    if not _is_spacy_model_installed(lang):
        _spacy_model_install(lang)
        from time import sleep
        sleep(1)
    if lang == 'de':
        spacy_lang = spacy.de.German
    elif lang == 'en':
        spacy_lang = spacy.en.English
    else:
        spacy_lang = spacy.en.English
        logger.error('unsupported language, falling back to English')
    vocab = spacy.vocab.Vocab(lex_attr_getters=spacy_lang.Defaults.lex_attr_getters,
                              tag_map=spacy_lang.Defaults.tag_map,
                              lemmatizer=spacy_lang.Defaults.create_lemmatizer())
    model = spacy.load(lang, vocab=vocab, add_vectors=False,
                       tagger=False, parser=False,
                       entity=False, matcher=False)
    model = spacy.load(lang, matcher=None, add_vectors=None)  # parser=None
    logger.info('...spacy model lang={} loaded'.format(lang))
    return model


_spacy_de = None
_spacy_en = None

# Spacy tokenizer does not split at infix '-' and '--' in German but
# it should, at least in our case (think "TXL-LHR"); this is a fix for that
_de_infix_expr = r'''\.\.\.
(?<=\p{L})\.(?=\p{L})
(?<=\p{L}):(?=\p{L})
(?<=\p{L})>(?=\p{L})
(?<=\p{L})<(?=\p{L})
(?<=\p{L})=(?=\p{L})
(?<=\p{L})-(?=\p{L})
(?<=\p{L})–(?=\p{L})
(?<=\p{L})--(?=\p{L})
(?<=\p{L})_(?=\p{L})'''
spacy_de_infix_fix = regex.compile('|'.join(_de_infix_expr.split('\n'))).finditer

_en_infix_expr = r'''\.\.\.+
(?<=[a-z])\.(?=[A-Z])
(?<=\p{L})-(?=\p{L})
(?<=\p{L})–(?=\p{L})
(?<=\p{L})--(?=\p{L})
(?<=\p{N})-(?=\p{N})
(?<=\p{L})–(?=\p{L})
(?<=\p{L}),(?=\p{L})
(?<=\p{L})_(?=\p{L})'''
spacy_en_infix_fix = regex.compile('|'.join(_en_infix_expr.split('\n'))).finditer


def _init_spacy():
    global _spacy_de, _spacy_en

    if not _spacy_de:
        _spacy_de = _spacy_model_load('de')
        if not type(_spacy_de) is str:
            # hack to make tests work which mock away spacy
            _spacy_de.tokenizer.infix_finditer = spacy_de_infix_fix
    if not _spacy_en:
        _spacy_en = _spacy_model_load('en')
        if not type(_spacy_en) is str:
            _spacy_en.tokenizer.infix_finditer = spacy_en_infix_fix


class SpacyTransformer(Transformer):
    def __init__(self):
        super(SpacyTransformer, self).__init__(input_keys=['msg_trc', 'language'],
                                               output_key='spacy')
        _init_spacy()

    def tag(self, msg, language):
        spacy = {
            'subject': {},
            'body': {}
        }
        # Make sure models are loaded
        if language == 'en':
            sp = _spacy_en
        else:
            sp = _spacy_de

        spacy['subject'] = self._spacy_to_ann(sp(msg['subject']))
        spacy['body'] = self._spacy_to_ann(sp(msg['body']))

        return spacy

    def _spacy_to_ann(self, doc):
        return [{
            'start': sent.start,
            'end': sent.end,
            'tokens': [{
                # There is more information attached to each token
                # which we might extract later (e.g. word vectors)
                'start': token.idx,
                'length': len(token),
                'text': token.orth_,
                'after': token.whitespace_,
                'pos': token.pos_,
                'tag': token.tag_,
                'ner': token.ent_type_,
                'ner_iob': token.ent_iob_,
                'shape': token.shape_
            } for token in sent]
            # 'vector': s.vector
        } for sent in doc.sents]

    def transform(self, *args):
        res = [None] * len(args[0])
        for idx, (msg, language,) in enumerate(zip(*args)):
            res[idx] = self.tag(msg, language)
        return res
