import spacy
import spacy.about as about
import sputnik
import logging
import regex
from sputnik.package_list import PackageNotFoundException, CompatiblePackageNotFoundException
from . helpers import tag_wrapper, TagStatus


logger = logging.getLogger(__name__)


def _is_spacy_model_installed(lang):
    try:
        sputnik.package(about.__title__, about.__version__, about.__models__[lang])
        return True
    except (PackageNotFoundException, CompatiblePackageNotFoundException):
        return False


def _spacy_model_install(lang):
    logger.info('Installing spacy models for lang={}... hold on'.format(lang))
    sputnik.install(about.__title__, about.__version__, about.__models__[lang])
    try:
        sputnik.package(about.__title__, about.__version__, about.__models__[lang])
    except (PackageNotFoundException, CompatiblePackageNotFoundException):
        logger.error("Spacy model for lang = {} failed to install".format(lang))
        raise Exception("Installing spacy model for lang={} failed".format(lang))


def _spacy_model_load(lang):
    logger.info('Loading spacy model for lang={}...'.format(lang))
    if not _is_spacy_model_installed(lang):
        _spacy_model_install(lang)
    from time import sleep
    sleep(1)
    model = spacy.load(lang)
    logger.info('...spacy model lang={} loaded'.format(lang))
    return model


_spacy_de = None
_spacy_en = None


# Spacy tokenizer does not split at infix '-' and '--' in German but
# it should, at least in our case (think "TXL-LHR"); this is a fix for that
_de_infix_expr = r'''\.\.\.
(?<=\p{L})\.(?=\p{L})
(?<=\p{L}):(?=\p{L})
(?<=\p{L})>(?=\p{L})
(?<=\p{L})<(?=\p{L})
(?<=\p{L})=(?=\p{L})
(?<=\p{L})-(?=\p{L})
(?<=\p{L})--(?=\p{L})'''
spacy_de_infix_fix = regex.compile('|'.join(_de_infix_expr.split('\n'))).finditer


def _init_spacy():
    global _spacy_de, _spacy_en

    if _spacy_de and _spacy_en:
        # Both models are loaded, return quickly
        return
    if not _spacy_de:
        _spacy_de = _spacy_model_load('de')
        if not type(_spacy_de) is str:
            # hack to make tests work which mock away spacy
            _spacy_de.tokenizer.infix_finditer = spacy_de_infix_fix
    if not _spacy_en:
        _spacy_en = _spacy_model_load('en')


def _spacy_to_ann(doc):
    return [{
        'start': sent.start,
        'end': sent.end,
        'tokens': [{
            # There is more information attached to each token
            # which we might extract later (e.g. word vectors)
            'start': token.idx,
            'length': len(token),
            'text': token.orth_,
            'after': token.whitespace_,
            'pos': token.pos_,
            'tag': token.tag_,
            'ner': token.ent_type_,
            'ner_iob': token.ent_iob_,
            'shape': token.shape_
        } for token in sent]
        # 'vector': s.vector
    } for sent in doc.sents]


@tag_wrapper("get_spacy")
def get_spacy(msg, result):
    result['spacy'] = {
        'subject': {},
        'body': {}
    }
    # Make sure models are loaded
    _init_spacy()
    if result['language'] == 'en':
        sp = _spacy_en
    else:
        sp = _spacy_de

    result['spacy']['subject'] = _spacy_to_ann(sp(msg['subject']))
    result['spacy']['body'] = _spacy_to_ann(sp(msg['body']))

    return TagStatus.success
