import regex as re
import numpy as np
from nltk.tokenize import wordpunct_tokenize
from nltk.stem import SnowballStemmer
from scipy.sparse import hstack
from sklearn.pipeline import BaseEstimator, TransformerMixin
from sklearn.feature_extraction.text import CountVectorizer, VectorizerMixin


def _get_body_lim(x):
    start_body = max([s['end'] for a in x['ann']['tags']
                      if a['type'] == 'Greeting' for s in a['spans']],
                     default=0)
    end_body = min([s['start'] for a in x['ann']['tags']
                    if a['type'] == 'EndOfMsg' for s in a['spans']],
                   default=len(x['msg']['body'])) - 1
    return start_body, end_body


class RichFeatureExtractor(BaseEstimator, TransformerMixin):
    """Extract "rich" features from a message like the number of mentioned
    locations etc.
    """
    def fit(self, X, y=None):
        return self

    def transform(self, data):
        features = np.recarray(shape=(len(data), 1),
                               dtype=[('len_body', 'int64'),
                                      ('len_subject', 'int64'),
                                      ('n_cc', 'int64'),
                                      ('n_to', 'int64'),
                                      ('n_per', 'int64'),
                                      ('n_per_crm', 'int64'),
                                      ('n_loc', 'int64'),
                                      ('n_loc_iata', 'int64')])
        for i, x in enumerate(data):
            start_body, end_body = _get_body_lim(x)
            ann_in_body = [a for a in x['ann']['tags'] if
                           min([s['end'] for s in a['spans']], default=0) >= start_body and
                           max([s['start'] for s in a['spans']],
                               default=len(x['msg']['body'])) <= end_body]
            features['len_body'][i] = len(x['msg']['body'])
            features['len_subject'][i] = len(x['msg']['subject'])
            features['n_cc'][i] = len(x['msg']['cc'])
            features['n_to'][i] = len(x['msg']['to'])
            features['n_per'][i] = len([a for a in ann_in_body if a['type'] == 'Person'])
            features['n_per_crm'][i] = len([a for a in ann_in_body if a['type'] == 'Person' and
                                            'crm_ids' in a['attributes']])
            features['n_loc'][i] = len([a for a in ann_in_body if a['type'] == 'Location'])
            features['n_loc_iata'][i] = len([a for a in ann_in_body if a['type'] == 'Location' and
                                             'iata_code' in a['attributes']])
        return features.view(np.int64)


class VectFeatureExtractor(BaseEstimator, VectorizerMixin):
    """Apply a bag of words feature extractor to subject and body
    separately (aka CountVectorizer from sklearn)
    
    Subject and body are preprocessed by wordpunct tokenizer, all
    tokens not pure letters [a-z] are dropped, also all tokens shorter
    than 3 characters. Finally the nltk Snowball stemmer in the
    appropriate language is applied to each token.

    For parameters see CountVectorizer of sklearn.
    """
    def __init__(self, min_df=1, ngram_range=(1, 1), binary=False, strip_accents=None):
        self.min_df = min_df
        self.ngram_range = ngram_range
        self.binary = binary
        self.stip_accents = strip_accents
        self.stemmer = SnowballStemmer('german')
        self.re_only_string = re.compile(r'^[a-z]{2,}$')
        self.body_vect = CountVectorizer(min_df=min_df, ngram_range=ngram_range,
                                         binary=binary, strip_accents=strip_accents,
                                         tokenizer=self.tokenizer)
        self.subject_vect = CountVectorizer(min_df=min_df, ngram_range=ngram_range,
                                            binary=binary, strip_accents=strip_accents,
                                            tokenizer=self.tokenizer)

    def tokenizer(self, s):
        return [st for st in [self.stemmer.stem(w) for w in wordpunct_tokenize(s)]
                if self.re_only_string.match(st)]

    def _get_msg_txt(self, x, what):
        if what == 'subject':
            return x['msg']['subject']
        else:
            start_body, end_body = _get_body_lim(x)
            if what == 'body':
                return x['msg']['body'][start_body:end_body]
            elif what == 'both':
                return x['msg']['subject'] + ' ' + x['msg']['body'][start_body:end_body]
            else:
                raise Exception("unknown what in get_msg_txt")

    def _get_body(self, X):
        return [self._get_msg_txt(x, what='body') for x in X]

    def _get_subject(self, X):
        return [self._get_msg_txt(x, what='subject') for x in X]

    def fit(self, X, y=None):
        self.subject_vect.fit(self._get_subject(X), y)
        self.body_vect.fit(self._get_body(X), y)
        return self

    def fit_transform(self, X, y=None):
        return hstack([self.subject_vect.fit_transform(self._get_subject(X), y),
                       self.body_vect.fit_transform(self._get_body(X), y)])

    def transform(self, X):
        return hstack([self.subject_vect.transform(self._get_subject(X)),
                       self.body_vect.transform(self._get_body(X))])
