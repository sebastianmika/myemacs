import os
import pickle
import numpy as np
import regex
import time

from collections import Counter
from keras.models import Sequential
from keras.optimizers import SGD
from keras.layers import LSTM, Merge, Embedding, Activation, TimeDistributed, Dense
from keras.utils.np_utils import to_categorical

from conlleval import conlleval


conf = {
    'language': 'de',
    'window': 5,
    'learning_rate': 0.05,
    'epochs': 100,
    'hidden': 50,
    'cv_conf': {
        'outer_folds': 2,
        'inner_folds': 3,
        'test_size': 0.1,
        'val_size': 0.1,
        'seed': 0
    },
    'embedding_conf': {
        'text': {'n': 100,
                 'replace_digits': True,
                 'lowercase': True,
                 'min_tc': 2},
        'shape': {'n': 5,
                  'replace_digits': False,
                  'lowercase': False,
                  'min_tc': 2},
        'pos': {'n': 5,
                'replace_digits': False,
                'lowercase': False,
                'min_tc': 2},
        'tag': {'n': 5,
                'replace_digits': False,
                'lowercase': False,
                'min_tc': 2},
        'ner': {'n': 5,
                'replace_digits': False,
                'lowercase': True,
                'min_tc': 2},
        'rner': {'n': 5,
                 'replace_digits': False,
                 'lowercase': False,
                 'min_tc': 2}
    }
}

# Load data
home = os.path.expanduser("~")
with open(os.path.join(home, 'e2e_eval.pkl'), 'rb') as f:
    data = pickle.load(f)

data = [d for d in data if d['ann']['language'] == conf['language'] and
        'booking' in d['ann']['label']]

for r in data:
    offset = 0
    new_ann = []
    skip = False
    for a in r['ann']['annotations']:
        if a['where'] == 'subject':
            new_ann.append(a)
        else:
            if a['rner'][2:] == 'Greeting':
                offset = a['start'] + a['length']
            elif a['rner'][2:] == 'EndOfMsg':
                skip = True
                continue
            elif a['rner'][2:] == 'Reply':
                raise Exception("This does lukily not happend")
            elif skip:
                continue
            else:
                a['start'] -= offset
                new_ann.append(a)
    r['ann']['annotations'] = new_ann


def generate_cv_splits(N, outer_folds=3, inner_folds=3, test_size=0.2, val_size=0.2, seed=1234):
    # Setup splits: train, validation and test
    np.random.seed(seed)
    cv_splits = []
    for outer_fold in range(outer_folds):
        train_test_split = np.random.permutation(N)
        i = int(N * test_size)
        test_idx = train_test_split[:i]
        train_idx = train_test_split[i:]
        splits = []
        for inner_fold in range(inner_folds):
            train_val_split = np.random.permutation(train_idx)
            i = int(len(train_idx) * val_size)
            splits.append((train_val_split[i:], train_val_split[:i]))
        cv_splits.append((splits, test_idx))
    return {'seed': seed,
            'splits': cv_splits}


def build_vocab(data, field, min_tc=2, replace_digits=False, lowercase=False,
                replace_dots=False, **kwargs):
    rep_dig = regex.compile('[0-9]')
    rep_dot = regex.compile('\.')

    def rep_digits(w):
        return rep_dig.sub('DIGIT', w)

    def rep_dots(w):
        return rep_dot.sub('-', w)

    def get_map_w(replace_digits, replace_dots, lowercase):
        def map_w_(w):
            if lowercase:
                w = w.lower()
            if replace_digits:
                w = rep_digits(w)
            if replace_dots:
                w = rep_dots(w)
            return w
        return map_w_

    def word2idx_(d, map_w):
        def get_idx(w):
            return d.get(map_w(w), 2)
        return get_idx

    map_w = get_map_w(replace_digits, replace_dots, lowercase)
    vocab = Counter([map_w(token[field]) for msg in data for token in msg['ann']['annotations']])
    vocab = [v for v, cnt in vocab.items() if cnt >= min_tc]
    word2idx = word2idx_({word: i + 3 for i, word in enumerate(vocab)}, map_w)
    idx2word = ['STARTPAD', 'ENDPAD', 'UNKNOWN'] + list(vocab)
    return {'field': field, 'vocab': vocab, 'word2idx': word2idx, 'idx2word': idx2word}


def contextwin(l, win):
    '''
    cf. https://github.com/mesnilgr/is13/blob/master/utils/tools.py
    win :: int corresponding to the size of the window
    given a list of indexes composing a sentence
    it will return a list of list of indexes corresponding
    to context windows surrounding each word in the sentence
    '''
    assert (win % 2) == 1
    assert win >= 1
    l = list(l)

    lpadded = win//2 * [0] + l + win//2 * [1]
    out = [lpadded[i:i+win] for i in range(len(l))]

    assert len(out) == len(l)
    return out


def embed_data(data, vocab, idx, win):
    return [np.array(contextwin(
        [vocab['word2idx'](t[vocab['field']]) for t in data[i]['ann']['annotations']],
        win)) for i in idx]


def setup_model(conf, n_classes, vocabs):
    # build input embedding layers
    embedding_layers = {}
    for field, embedding_conf in conf['embedding_conf'].items():
        model = Sequential(name=field)
        # input_length=conf['window']) removed until I understand what it does :-)
        layer = Embedding(len(vocabs[field]['idx2word']),
                          embedding_conf['n'],
                          name='emb_{}'.format(field))
        # Need to explicitely add the input layer, as it does not get a
        # proper name otherwise
        layer.create_input_layer(layer.batch_input_shape, layer.input_dtype, name=field)
        model.add(layer)
        embedding_layers[field] = {'model': model,
                                   'layer': layer}
    merge_input_layer = Merge([e['layer'] for e in embedding_layers.values()],
                              mode='concat')
    model = Sequential()
    model.add(merge_input_layer)
    model.add(LSTM(conf['hidden'], activation='sigmoid', return_sequences=True))
    model.add(TimeDistributed(Dense(output_dim=n_classes)))
    model.add(Activation("softmax"))

    sgd = SGD(lr=conf['learning_rate'], momentum=0.0, decay=0.0, nesterov=False)
    model.compile(loss='categorical_crossentropy', optimizer=sgd,
                  metrics=['accuracy'])
    return model


def generate_vocabs(in_data, conf, test_idx):
    # build vocabularies for embedding layers
    vocabs = {}
    data = [d for i, d in enumerate(in_data) if i not in test_idx]
    for field, conf_vocab in conf['embedding_conf'].items():
        vocabs[field] = build_vocab(data, field, **conf_vocab)
    labels = build_vocab(data, 'label', min_tc=10, replace_digits=False, lowercase=False,
                         replace_dots=True)
    return vocabs, labels


def generate_data(data, vocabs, conf):
    X = {field: [np.array([vocabs[field]['word2idx'](token[field])
                           for token in d['ann']['annotations']])[np.newaxis, :]
                 for d in data]
         for field, conf_vocab in conf['embedding_conf'].items()}
    y = [np.array([labels['word2idx'](t['label']) for t in d['ann']['annotations']]) for d in data]
    return X, y

cv_splits = generate_cv_splits(len(data), **conf['cv_conf'])

for outer_fold, cv in enumerate(cv_splits['splits']):
    te_idx = cv[1]
    print('OF {}: generating vocabularies'.format(outer_fold))
    # I generate the vocabulary in the training AND validation set;
    # probably that should only be done on the training set
    vocabs, labels = generate_vocabs(data, conf, te_idx)
    X, y = generate_data(data, vocabs, conf)
    n_classes = len(labels['idx2word'])
    for inner_fold, (tr_idx, val_idx) in enumerate(cv[0]):
        print('OF {} IF {}: Setting up'.format(outer_fold, inner_fold))
        # A fresh model per inner loop
        model = setup_model(conf, n_classes, vocabs)
        print('OF {} IF {}: model has {} parameters'.format(
            outer_fold, inner_fold, model.count_params()))
        best_f1 = 0.0
        # Run epochs as full loops over the training data
        for epoch in range(conf['epochs']):
            # Shuffle training data
            tr_idx_shuffle = np.random.permutation(tr_idx)
            tic = time.time()
            # Sweep over training data once
            for idx, i in enumerate(tr_idx_shuffle):
                X_batch = {field: x[i] for field, x in X.items()}
                y_batch = to_categorical(y[i], n_classes)[np.newaxis, :, :]
                model.train_on_batch(X_batch, y_batch)
                if idx % 100 == 0 or idx == len(tr_idx_shuffle) - 1:
                    print('OFÂ {} IF {} E {:2d}: {:.2%} completed in {:.2f} (sec)\r'.format(
                        outer_fold, inner_fold, epoch, (idx+1) / len(tr_idx_shuffle),
                        time.time()-tic), end="", flush=True)
            # Evaluate performance on test and validation data
            res = {}
            print('')
            for what, idx in zip(['validation', 'test'], [val_idx, te_idx]):
                preds = [list(map(lambda x: labels['idx2word'][x],
                                  model.predict({field: x[i]
                                                 for field, x in X.items()}).argmax(2)[0]))
                         for i in val_idx]
                truth = [map(lambda x: labels['idx2word'][x], y[i]) for i in val_idx]
                # No ws in words, but we have ws-tokens
                r = regex.compile('\s')
                words = [map(lambda x: r.sub('WS', vocabs['text']['idx2word'][x]),
                             X['text'][i][0, :]) for i in val_idx]
                res[what] = conlleval(preds, truth, words, 'test.txt')
                print('OF {outer_fold} IF {inner_fold} E {epoch:2d} {what:10s} error: '
                      'precision {precision:5.2f}% '
                      'recall {recall:5.2f}% accuracy: {accuracy:5.2f}% '
                      'f1: {f1:5.2f}'.format(
                          outer_fold=outer_fold, inner_fold=inner_fold, 
                          epoch=epoch, what=what, **res[what]))
