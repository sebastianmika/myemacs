import pandas as pd
import csv
from math import log10
import requests
import json
import sys
import pickle


csv.field_size_limit(sys.maxsize)
base_uri = 'http://192.168.99.100:9200/'
load_saved = True

#
# Setup es
#
print("Regenerating index...")

# Delete index
requests.delete('{}/geonames'.format(base_uri))

# Create index
requests.put('{}/geonames'.format(base_uri),
             data=json.dumps({
                 "settings": {
                     "analysis": {
                         "filter": {
                             "filter_shingle": {
                                 "type": "shingle",
                                 "max_shingle_size": 3,
                                 "min_shingle_size": 2,
                                 "output_unigrams": True
                             },
                             "filter_stop": {
                                 "type": "stop",
                                 "stopwords": ["airport", "air base", "ab", "flughafen"]
                             }
                         },
                         "analyzer": {
                             # "analyzer_keyword": {
                             #     "type": "custom",
                             #     "tokenizer": "keyword",
                             #     "filter": [
                             #         "asciifolding",
                             #         "lowercase"
                             #     ]
                             # },
                             "analyzer_shingle": {
                                 "type": "custom",
                                 "tokenizer": "standard",
                                 "filter": [
                                     "asciifolding",
                                     "lowercase",
                                     "filter_stop",
                                     "filter_shingle"
                                 ]
                             }
                         }
                     }
                 },
                 "mappings": {
                     "places": {
                         "properties": {
                             "name": {
                                 "type": "string",
                                 "analyzer": "analyzer_shingle"
                                 # "search_analyzer": "analyzer_shingle"
                             },
                             "alternate_names": {
                                 "type": "string",
                                 "analyzer": "analyzer_shingle"
                                 # "search_analyzer": "analyzer_shingle"
                             },
                             "parent": {
                                 "type": "integer"
                             },
                             "location": {
                                 "type": "geo_point"
                             },
                             "iata_codes": {
                                 "type": "string",
                                 # do not lowercase, no tokenization required, etc.
                                 "index": "not_analyzed"
                             },
                             "feature_code": {
                                 "type": "string",
                                 "index": "not_analyzed"  # want to do wildcard expre on this
                             },
                             "feature_class": {
                                 "type": "string",
                                 "index": "not_analyzed"  # want to do wildcard expre on this
                             },
                             "admin1_code": {
                                 "type": "string",
                                 "index": "not_analyzed"
                             },
                             "admin2_code": {
                                 "type": "string",
                                 "index": "not_analyzed"
                             },
                             "admin3_code": {
                                 "type": "string",
                                 "index": "not_analyzed"
                             },
                             "admin4_code": {
                                 "type": "string",
                                 "index": "not_analyzed"
                             },
                             "country_code": {
                                 "type": "string",
                                 "index": "not_analyzed"
                             },
                             "cc2": {
                                 "type": "string",
                                 "index": "not_analyzed"
                             }
                         }
                     }
                 }
             }))

names = [
    'geoname_id',  # integer id of record in geonames database
    'name',  # name of geographical point (utf8) varchar(200)
    'ascii_name',  # name of geographical point in plain ascii
                   # characters, varchar(200)
    'alternate_names',  # alternatenames, comma separated, ascii names
                        # automatically transliterated, convenience
                        # attribute from alternatename table,
                        # varchar(10000)
    'latitude',  # latitude in decimal degrees (wgs84)
    'longitude',  # longitude in decimal degrees (wgs84)
    'feature_class',  # see http://www.geonames.org/export/codes.html,
                      # char(1)
    'feature_code',  # see http://www.geonames.org/export/codes.html,
                     # varchar(10)
    'country_code',  # ISO-3166 2-letter country code, 2 characters
    'cc2',  # alternate country codes, comma separated, ISO-3166
            # 2-letter country code, 200 characters
    'admin1_code',  # fipscode (subject to change to iso code), see
                    # exceptions below, see file admin1Codes.txt for
                    # display names of this code; varchar(20)
    'admin2_code',  # code for the second administrative division, a
                    # county in the US, see file admin2Codes.txt;
                    # varchar(80)
    'admin3_code',  # code for third level administrative division,
                    # varchar(20)
    'admin4_code',  # code for fourth level administrative division,
                    # varchar(20)
    'population',  # bigint (8 byte int)
    'elevation',  # in meters, integer
    'dem',  # digital elevation model, srtm3 or gtopo30, average
            # elevation of 3''x3'' (ca 90mx90m) or 30''x30'' (ca
            # 900mx900m) area in meters, integer. srtm processed by
            # cgiar/ciat.
    'timezone',  # the timezone id (see file timeZone.txt) varchar(40)
    'modification_date'   # date of last modification in yyyy-MM-dd format
]

#
# Preload alternate names and get iata codes
#
print("Preloading alternate names...")
if load_saved:
    with open('alternate_names.pkl', 'rb') as f:
        alternate_names = pickle.load(f)
        iata_codes_per_geoname_id = pickle.load(f)
else:
    iata_codes_per_geoname_id = {}
    alternate_names = {}
    with open('../../../geonames/alternateNames.txt', 'r', newline='') as f:
        for row in csv.reader(f, delimiter='\t'):
            gid = int(row[1])
            if row[2] == 'iata' and row[7] == '':
                # iata not historic
                if gid not in iata_codes_per_geoname_id:
                    iata_codes_per_geoname_id[gid] = set()
                iata_codes_per_geoname_id[gid].add(row[3])
            elif ((row[4] == '1' or row[5] == '1') and
                  (row[2] == 'de' or row[2] == 'en') and
                  (row[7] == '')):
                # only preferred + short in en and de and not historical ones
                if gid not in alternate_names:
                    alternate_names[gid] = set()
                alternate_names[gid].add(row[3])
    with open('alternate_names.pkl', 'wb') as f:
        pickle.dump(alternate_names, f)
        pickle.dump(iata_codes_per_geoname_id, f)

#
# Get data to build up parent relation
#
print("Building code_data...")
if load_saved:
    with open('code_data.pkl', 'rb') as f:
        code_data = pickle.load(f)
        # code_data_pd = pickle.load(f)
else:
    cnt = 0
    code_data = {}
    with open('../../../geonames/allCountries.txt', 'r', newline='', encoding='utf-8') as f:
        for row in csv.reader(f, delimiter='\t', quoting=csv.QUOTE_NONE):
            cnt += 1
            if cnt % 1000000 == 0:
                print("at {}".format(cnt))
            rr = dict(zip(names, row))
            rr['geoname_id'] = int(rr['geoname_id'])
            rr['country_code'] = rr['country_code'].lower()
            key = ("{country_code}.{feature_class}.{feature_code}."
                   "{admin1_code}.{admin2_code}.{admin3_code}.{admin4_code}").format(
                       **rr)
            if key not in code_data:
                code_data[key] = [rr['geoname_id']]
            else:
                code_data[key].append(rr['geoname_id'])

    with open('code_data.pkl', 'wb') as f:
        pickle.dump(code_data, f)
        # code_data = pd.DataFrame([k.split('.') + [v] for k, v in code_data.items()],
        #                            columns=['cc', 'fcl', 'fcd', 'a1', 'a2', 'a3', 'a4', 'ids'])


def resolve_parent(p, code_data):

    def get_one(query, gid):
        res = code_data.query(query)
        if res.shape[0] != 1:
            # something is strange...
            print("query for {} = {} returned != 1 results: {}".format(gid, query, res.shape[0]))
            return None
        else:
            return res.iloc[0].ids[0]

    def get_one_d(key, gid):
        res = code_data.get(key, None)
        if res is None:
            print("query for {} = {} returned nothing".format(gid, key))
            return None
        if len(res) != 1:
            # something is strange...
            print("query for {} = {} returned != 1 results: {}".format(gid, key, res))
            return None
        else:
            return res[0]
        
    if p['country_code'] == '':
        # things without a country code will not be attached any parents
        return

    if p['feature_code'] == 'PCLI':
        # this IS a country/"independent political entity"; as of time
        # of writing there are 193 of these in geonames, matching the
        # 194 countries listed in wikipedia (did not check which one
        # is missing)
        #
        # a country has the parent world
        p['parent'] = 'world'
    elif p['feature_code'].startswith('ADM1'):
        # An ADM1/H entitiy, find the country
        p['parent'] = get_one_d(("{country_code}.A.PCLI.00...").format(**p),
                                p['geoname_id'])
        # p['parent'] = get_one("cc=='{country_code}' and "
        #                       "fcl=='A' and fcd=='PCLI'".format(**p),
        #                       p['geoname_id'])
    elif p['feature_code'].startswith('ADM2'):
        # An ADM2/H entitiy, find the ADM1
        p['parent'] = get_one_d(("{country_code}.A.ADM1.{admin1_code}...").format(**p),
                                p['geoname_id'])
        # p['parent'] = get_one("cc=='{country_code}' and "
        #                       "fcl=='A' and "
        #                       "fcd=='ADM1' and "
        #                       "a1=='{admin1_code}'".format(**p),
        #                       p['geoname_id'])
    elif p['feature_code'].startswith('ADM3'):
        # An ADM2/H entitiy, find the ADM1
        p['parent'] = get_one_d(("{country_code}.A.ADM2.{admin1_code}.{admin2_code}..").format(**p),
                                p['geoname_id'])
        # p['parent'] = get_one("cc=='{country_code}' and "
        #                       "fcl=='A' and "
        #                       "fcd=='ADM2' and "
        #                       "a1=='{admin1_code}' and "
        #                       "a2=='{admin2_code}'".format(**p),
        #                       p['geoname_id'])
    elif p['feature_code'].startswith('ADM4'):
        # An ADM2/H entitiy, find the ADM1
        p['parent'] = get_one_d(("{country_code}.A.ADM3.{admin1_code}.{admin2_code}.{admin3_code}.").format(**p),
                                p['geoname_id'])
        # p['parent'] = get_one("cc=='{country_code}' and "
        #                       "fcl=='A' and "
        #                       "fcd=='ADM3' and "
        #                       "a1=='{admin1_code}' and "
        #                       "a2=='{admin2_code}' and "
        #                       "a3=='{admin3_code}'".format(**p),
        #                       p['geoname_id'])
    else:
        # not an ADM code - find lowest adminX code set and get feature_code=ADMX
        if p['admin4_code']:
            feature_code = 'ADM4'
        elif p['admin3_code']:
            feature_code = 'ADM3'
        elif p['admin2_code']:
            feature_code = 'ADM2'
        elif p['admin1_code']:
            feature_code = 'ADM1'
        else:
            print('could not find parent ADM code for {}'.format(p))
            return
        p['parent'] = get_one_d(("{country_code}.A.{feature_code_new}.{admin1_code}.{admin2_code}.{admin3_code}.{admin4_code}").format(feature_code_new=feature_code, **p),
                                p['geoname_id'])
        # p['parent'] = get_one("cc=='{country_code}' and "
        #                       "fcl=='A' and "
        #                       "fcd=='{feature_code_new}' and "
        #                       "a1=='{admin1_code}' and "
        #                       "a2=='{admin2_code}' and "
        #                       "a3=='{admin3_code}' and "
        #                       "a4=='{admin4_code}'".format(feature_code_new=feature_code, **p),
        #                       p['geoname_id'])


def bulk_insert(payload):
    uri = '{}_bulk'.format(base_uri)
    res = requests.put(uri, data=payload, headers={'Content-Type': 'application/json'})
    assert res.status_code == 200


cnt = 0
blk_size = 100000
blk_cnt = 0
blk_payload = ""
print("Populating ES...")
with open('../../../geonames/allCountries.txt', 'r', newline='', encoding='utf-8') as f:
    for row in csv.reader(f, delimiter='\t', quoting=csv.QUOTE_NONE):
        cnt += 1
        if cnt % 10000 == 0:
            print("at {}".format(cnt))
        rr = dict(zip(names, row))
        rr['geoname_id'] = int(rr['geoname_id'])
        rr['population'] = max(int(rr['population']), 0)
        rr['log_population'] = log10(1.0 + int(rr['population'])) + 1.0
        rr['location'] = {'lat': float(rr['latitude']), 'lon': float(rr['longitude'])}
        rr.pop('latitude')
        rr.pop('longitude')
        rr['elevation'] = int(rr['elevation']) if rr['elevation'] else None
        an = alternate_names.get(rr['geoname_id'], set())
        rr['alternate_names'] = list(an)
        iata_code = iata_codes_per_geoname_id.get(rr['geoname_id'], set())
        rr['iata_codes'] = list(iata_code)
        rr['country_code'] = rr['country_code'].lower()
        rr['cc2'] = [cc.lower() for cc in rr['cc2']]
        rr['parent'] = resolve_parent(rr, code_data)
        if blk_cnt >= blk_size:
            bulk_insert(blk_payload)
            blk_payload = ""
            blk_cnt = 0
        else:
            blk_cnt += 1
            blk_payload += ('{"index": { "_index": "geonames", "_type": "places", "_id": "'
                            + str(rr['geoname_id']) + '"} }\n')
            blk_payload += json.dumps(rr) + '\n'
# insert final batch
if blk_cnt > 0:
    bulk_insert(blk_payload)
print("FINISHED - INSERTED {} DOCUMENTS".format(cnt))
