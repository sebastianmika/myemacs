import logging
import glob
import json
import os
import pandas as pd


logger = logging.getLogger(__name__)


def load_json_audit_logs(path):
    """Load zendesk ticket audit logs from s3

    Each file contains a sequence of json objects in the form
    "{...}{...}". The filename and the number of records are detemined
    by AWS firehose.

    Each entry is of the form {"ticketId": 1234, "audit_log": [...]}
    where "audit_log" contains the result of *one* call to the zendesk
    ticket audit API at GET /api/v2/tickets/{ticket_id}/audits.json
    for the ticketId.

    Within this array, each separate audit generated by zendesk has an
    'id'. We already de-dup entries in the audit logs over this 'id',
    i.e. we only keep one copy of each audit_log entry (should be the
    latest, but that is ignore here; for ES that would just amount to
    update based on the audit log 'id' field.

    Finally within each audit log there is a list of events. Per
    ticket id the concatenated list of events is returned, where each
    evenv is enriched with a field "audit" that contains the meta
    information of the encapsulating audit log (esp. the 'created_at',
    'id' and 'via'). These resulting super-events are sorted by the
    audit.created_at

    This could be stored in ES...
    """
    res = {}
    for fname in glob.glob('{}/**'.format(path), recursive=True):
        if not os.path.isfile(fname):
            continue
        with open(fname, 'r') as fid:
            txt = fid.read()
        jd = json.JSONDecoder()
        while txt:
            obj, idx = jd.raw_decode(txt)
            txt = txt[idx:]
            # Only take stuff where we know the audit log
            if 'audit_log' not in obj:
                continue
            obj['audit_log'] = sorted(obj['audit_log'],
                                      key=lambda x: pd.to_datetime(x['created_at']))
            # And only take tickets where the log begins after Sep 30th, 2016
            if (pd.to_datetime(obj['audit_log'][0]['created_at']) <
               pd.to_datetime('2016-10-01T00:00:00T')):
                continue
            ticketId = obj['ticketId']
            if ticketId not in res:
                res[ticketId] = {}
            # De-duplicate on the 'id' of the very zendesk log entry
            res[ticketId].update({l['id']: l for l in obj['audit_log']})
    # Flatten out audit log id level again
    for tid in res.keys():
        res[tid] = [dict({'event_idx': i}, **e) for i, e in enumerate(
                    sorted([dict({
                        'audit': {
                            # this is in principle just al.pop('events')
                            'via': al['via'],
                            'created_at': al['created_at'],
                            'id': al['id'],
                            'author_id': al['author_id'],
                            'metadata': al['metadata'],
                            'ticket_id': al['ticket_id']}
                    }, **e) for al in res[tid].values() for e in al['events']],
                           key=lambda x: x['audit']['created_at']))]
    # res[tid] = sorted(res[tid].values(), key=lambda x: pd.to_datetime(x['created_at']))
    return res


def load_json_offer_logs(path):
    """Load offer logs; get latest update to offer

    ticket_id/offer_id/created_at -> offer
    """
    res = {}
    for fname in glob.glob('{}/**'.format(path), recursive=True):
        if not os.path.isfile(fname):
            continue
        with open(fname, 'r') as fid:
            txt = fid.read()
        jd = json.JSONDecoder()
        while txt:
            obj, idx = jd.raw_decode(txt)
            txt = txt[idx:]
            if obj['status'] == 'draft':
                continue
            ticketId = int(obj['zendesk_ticket_id'])
            if ticketId not in res:
                res[ticketId] = {}
            if obj['_id'] not in res[ticketId]:
                res[ticketId][obj['_id']] = {obj['meta_data']['created_at']: obj}
            elif (obj['meta_data']['created_at'] in res[ticketId][obj['_id']] and
                  res[ticketId][obj['_id']][obj['meta_data']['created_at']]['__v'] < obj['__v']):
                res[ticketId][obj['_id']][obj['meta_data']['created_at']] = obj
            elif (obj['meta_data']['created_at'] in res[ticketId][obj['_id']] and
                  res[ticketId][obj['_id']][obj['meta_data']['created_at']]['__v'] >= obj['__v']):
                pass
            else:
                raise Exception("More than one offer with the same id at different times")
    return res


def load_json_booking_logs(path):
    res = {}
    for fname in glob.glob('{}/**'.format(path), recursive=True):
        if not os.path.isfile(fname):
            continue
        with open(fname, 'r') as fid:
            txt = fid.read()
        jd = json.JSONDecoder()
        while txt:
            obj, idx = jd.raw_decode(txt)
            txt = txt[idx:]
            if 'booking_items' not in obj:
                logger.warning('no booking items in {}'.format(fname))
                continue
            if obj['status'] == 'draft':
                # Do not load draft bookings
                continue
            if 'zendesk_ticket_id' in obj:
                ticketId = int(obj['zendesk_ticket_id'])
            else:
                ticketId = 'unknown'
            if ticketId not in res:
                res[ticketId] = {}
            if obj['_id'] not in res[ticketId]:
                res[ticketId][obj['_id']] = {bi['id']: bi for bi in obj['booking_items']}
            else:
                res[ticketId][obj['_id']].update({bi['id']: bi for bi in obj['booking_items']})
    return res


def get_users(user, pw):
    """Get the users data from Zendesk to resolve author_id fields"""
    import requests
    users = []
    uri = 'https://comtravo.zendesk.com/api/v2/users.json'
    while uri:
        res = requests.get(
                uri,
                auth=(user, pw),
                headers={'Accept': 'application/json'})
        if res.status_code != 200:
            logger.error("Error {} getting more users".format(res.status_code))
            return users
        users += res.json()['users']
        uri = res.json()['next_page']
    return users
