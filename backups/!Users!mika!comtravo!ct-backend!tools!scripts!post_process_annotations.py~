import os
import regex
from glob import glob
from copy import deepcopy
from datetime import datetime
from unittest.util import _count_diff_all_purpose
import json
import bz2
from brat_io import brat2ann
from email_tagger.merge_annotations import merge_spacy_with_annotations
from email_tagger.get_spacy import get_spacy


def load_raw_data(path):
    """Batch load all data at path. Recurses the directory at path and
    loads each file ending in *.ann. Result listis stored in a file
    raw_data.pkl. To speed up loading this file can directly be loaded
    by specifying load_from_file=True

    """
    raw_data = []
    ids = set()
    n_ann_total = 0
    for file in glob(os.path.join(path, '**/*.ann'), recursive=True):
        file_stem = file.replace(".ann", "")
        try:
            ann, _ = brat2ann(file_stem)
            if (ann['annotations'][0]['attributes']['request_type'] ==
               "booking|negotiation|rebooking|cancellation|other"):
                print('type not tagged?!? {}'.format(file))
            ann['label'] = ann['annotations'][0]['attributes']['request_type']
            ann['source'] = ann['annotations'][0]['attributes']['source']
            ann['language'] = ann['annotations'][0]['attributes']['language']
            ann['annotations'] = ann['annotations'][1:]
            n_ann_total += len(ann['annotations'])
            msg = json.load(open(file.replace(".ann", ".json"), 'r'))
            raw_data.append({'id': os.path.basename(file_stem),
                             'path': os.path.split(file_stem)[0],
                             'msg': msg,
                             'ann': ann})
            ids.add(os.path.basename(file_stem))
        except Exception as e:
            print("ERROR: {} - {}".format(file_stem, e))
    print("...got {} annotations for {} messages of which {} are unique".format(
        n_ann_total, len(raw_data), len(ids)))
    return raw_data


def ann_start(a):
    return min([s['start'] for s in a['spans']])


def ann_end(a):
    return max([s['end'] for s in a['spans']])


def ann_ws_sep(a, m):
    """Check that a (manual) annotation is surrounded by proper separator
    chars (i.e. does not start or end within a word"""
    sepchar = ' \t\n.,;!?()-©/:*+><[]{}„“"@´\''
    where = a['where']
    if where not in ['subject', 'body']:
        return False
    start = ann_start(a)
    end = ann_end(a)
    before = m['msg'][where][max(start-1, 0):start]
    after = m['msg'][where][end:min(end+1, len(m['msg'][where]))]
    return ((before in sepchar or before == '') and
            (after in sepchar or after == ''))


def ann_to_spacy(m):
    """Convert (manual) annotation to our internal annotations format
    based on spacy tokenization"""
    body_txt = m['msg']['body']
    subject_txt = m['msg']['subject']
    res = {
        'annotations': m['ann']['annotations_orig'],
        'analyzed': {
            'body': {
                'text': body_txt,
                'start': 0,
                'end': len(body_txt)
            },
            'subject': {
                'text': subject_txt,
                'start': 0,
                'end': len(subject_txt)}},
        'log': [],
        'language': m['ann']['language']
    }
    get_spacy(m['msg'], res)
    m['ann']['annotations'] = merge_spacy_with_annotations(res)


def label_from_events(m):
    """generate for events e of message m a label for each token in the
    annotations a

    labels per token have the form
    [B|I].<entity_type>.<event_type>.<event_type_cnt>.<event_role> or just O for no
    specific role
    """
    event_type_cnt = {}
    n_target_missing = 0
    # label all annotations with 'O'
    for a in m['ann']['annotations']:
        a['label'] = 'O'
    for e in m['ann']['events']:
        event_type = e['type']
        if event_type in event_type_cnt:
            event_type_cnt[event_type] += 1
        else:
            event_type_cnt[event_type] = 0
        # get all connected tokens
        for arg in e['args']:
            # role = name, but remove any trailing count
            event_role = regex.sub('(?r)[0-9]+', '', arg['name'], count=1)
            targets = [a for a in m['ann']['annotations']
                       if 'id' in a and a['id'] == arg['target']]
            if targets:
                # append labels
                for t in targets:
                    iob, entity_type = t['rner'].split('-')
                    t['label'] = '{}.{}.{}.{}.{}'.format(
                        iob, entity_type, event_type, event_type_cnt[event_type], event_role)
            else:
                # Ups - target not found. That should happen rarely,
                # actually only for the misaligned and removed
                # annotations
                n_target_missing += 1
    return n_target_missing


def clean_eom_reply(ann):
    """Remove all annotation that are after the start of the end of
    message or within a reply
    """
    orig_n_ann = len(ann)
    eom = [a for a in ann if a['type'] == 'EndOfMsg']
    rpl = [a for a in ann if a['type'] == 'Reply']
    min_eom_start = min([ann_start(e) for e in eom], default=None)
    if min_eom_start is not None:
        ann = [a for a in ann if ann_end(a) < min_eom_start] + eom
    if len(rpl) > 0:
        for r in rpl:
            r_start = ann_start(r)
            r_end = ann_end(r)
            ann = [a for a in ann if ann_end(a) < r_start or ann_start(a) > r_end]
    return ann, orig_n_ann - len(ann)


home = os.path.expanduser("~")
data = load_raw_data(os.path.join(home, 'comtravo/data/nlp/nlp_data'))
n_ann_cleaned = 0
for d in data:
    d['ann']['annotations'], n = clean_eom_reply(d['ann']['annotations'])
    n_ann_cleaned += n
print('...removed {} annotations from eom or replies'.format(n_ann_cleaned))

# pair messages and check "label" = request_type
msg_ids = list({m['id']: 0 for m in data}.keys())
label_mismatch = []
language_mismatch = []
annotation_mismatch = []
for msg_id in msg_ids:
    msg = [m for m in data if m['id'] == msg_id]
    if msg[0]['ann']['label'] != msg[1]['ann']['label']:
        label_mismatch.append(msg_id)
    if msg[0]['ann']['language'] != msg[1]['ann']['language']:
        language_mismatch.append(msg_id)
    a0 = [{k: v for k, v in a.items() if k != 'id' and k != 'attributes'
           and k != 'relates_to' and k != 'is_related_to'} for a in msg[0]['ann']['annotations']]
    a1 = [{k: v for k, v in a.items() if k != 'id' and k != 'attributes'
           and k != 'relates_to' and k != 'is_related_to'} for a in msg[1]['ann']['annotations']]
    d = _count_diff_all_purpose(a0, a1)
    if len(d) > 0:
        annotation_mismatch.append({'id': msg_id, 'diffs': d})
print('...label (request_type) mismatch for {} message pairs'.format(len(label_mismatch)))
print('...language mismatch for {} message pairs'.format(len(language_mismatch)))
print('...annotation mismatch for {} message pairs (excluding relations)'.format(
    len(annotation_mismatch)))

# check each annotation for overlaps of entities
annotation_overlaps = []
new_data = []
for m in data:
    drop = False
    for where in ['body', 'subject']:
        ann = deepcopy([a for a in m['ann']['annotations'] if a['where'] == where])
        ann = sorted(ann, key=lambda x: ann_start(x))
        for i in range(len(ann)):
            if (ann_start(ann[i]) < ann_end(ann[i-1]) if i > 0 else False or
               ann_end(ann[i]) > ann_start(ann[i+1]) if i < len(ann) - 1 else False):
                annotation_overlaps.append(m['id'])
                drop = True
                break
    if not drop:
        new_data.append(m)
print('...overlapping annotations in {} annotated messages (non unique)'.format(
    len(annotation_overlaps)))

#
# Clean up
#
print('-> deleting {} individual messages because of overlaps'.format(len(annotation_overlaps)))
data = new_data

# check each annotation whether it starts or end within a word
annotation_border_error = []
new_data = []
n_ann = 0
n_target_missing = 0
for m in data:
    print('. -> ', len(m['ann']['annotations']))
    new_ann = []
    for a in m['ann']['annotations']:
        n_ann += 1
        if ann_ws_sep(a, m):
            new_ann.append(a)
        else:
            annotation_border_error.append(('{} - {}'.format(
                m['id'], a['text'])))
    m['ann']['annotations_orig'] = new_ann
    ann_to_spacy(m)
    n_target_missing += label_from_events(m)
print('...border errors or tags not in body/subject annotations: {} of {} got deleted '
      '(causing probably issues with events!)'.format(
          len(annotation_border_error), n_ann))
print('...{} targets in events which are not in annotations'.format(n_target_missing))

with open('annotation_border_error.txt', 'w') as f:
    f.write('\n'.join(annotation_border_error))

# drop all messages which have:
# - label mismatches
# - language mismatches
drop_ids = list({k: 0 for k in label_mismatch + language_mismatch}.keys())
print('-> deleting {} message pairs because of label/language mismatches'.format(len(drop_ids)))
data = [d for d in data if d['id'] not in drop_ids]
print('RESULT: {} messages, {} unique messages'.format(
    len(data),
    len(list({m['id']: 0 for m in data}.keys()))))

annotation_overlap = sorted(annotation_overlaps)
with open('annotation_overlaps.csv', 'w') as f:
    f.write("msg_id;resolved\n")
    for l in annotation_overlaps:
        f.write("{};\n".format(l))
label_mismatch = sorted(label_mismatch)
language_mismatch = sorted(language_mismatch)
with open('label_mismatch.csv', 'w') as f:
    f.write("msg_id;resolved\n")
    for l in label_mismatch:
        f.write("{};\n".format(l))
with open('language_mismatch.csv', 'w') as f:
    f.write("msg_id;resolved\n")
    for l in language_mismatch:
        f.write("{};\n".format(l))

ts = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')
fname = os.path.join(home, 'comtravo/data/nlp/nlp_data/annotated_corpus_{}.json.bz2'.format(ts))
print('WRITING to {}'.format(fname))
with bz2.BZ2File(fname, 'wb') as f:
    f.write(json.dumps(data, indent=2).encode())

# count number of remained annotations
msg_ids = list({m['id']: 0 for m in data}.keys())
n_total_ann = 0
n_per_unique_msg_ann = 0
for msg_id in msg_ids:
    msg = [d for d in data if d['id'] == msg_id]
    n_this = 0
    for m in msg:
        n_this += len(m['ann']['annotations'])
        n_total_ann += len(m['ann']['annotations'])
    n_per_unique_msg_ann += float(n_this)/len(msg)
print('FINALLY we have {} total annotations and approx. {} annotations in unique messages'.format(
    n_total_ann, n_per_unique_msg_ann))
